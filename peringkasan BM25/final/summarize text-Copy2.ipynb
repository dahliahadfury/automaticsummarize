{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#bsoup\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# import StemmerFactory class\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function does cleaning, tokenize, remove stopwords, and stemming\n",
    "def get_clean_corpus(corpus, stopwords):\n",
    "    \n",
    "    #segmentasi\n",
    "    temp = sent_tokenize(corpus)\n",
    "    corpus = pd.DataFrame(temp, columns=['dokumen'])\n",
    "    \n",
    "    clean_corpus = []\n",
    "    token = []\n",
    "    for index, sentence in enumerate(corpus['dokumen']):\n",
    "        term = word_tokenize(corpus['dokumen'][index])\n",
    "        \n",
    "        #deleting url\n",
    "        deleted_url = [temp for temp in term if not \n",
    "                       re.match(r\"\\w+(?:(\\.(\\w+)\\.(\\w+)))|\\w+(?:(\\.(\\w+)))\", str(temp))]\n",
    "        \n",
    "        #deleting symbol\n",
    "        deleted_symbol = [re.sub(r\"[\\-\\+\\=\\:\\;\\\"\\\\\\@\\[\\]\\,_!;.':#$%^&*()<>?/\\|}{~:]\",\" \",\n",
    "                                 str(temp)) for temp in deleted_url ]\n",
    "        \n",
    "        #stemming\n",
    "        stemmed_sentence = stemmer.stem(\" \".join(deleted_symbol))\n",
    "        \n",
    "        #tanpa stemming\n",
    "#         stemmed_sentence2 = (\" \".join(deleted_symbol))\n",
    "        \n",
    "        tokens = word_tokenize(stemmed_sentence)\n",
    "        for i in range(len(tokens)):\n",
    "            for index, word in enumerate(tokens):\n",
    "                #delete stopwprds\n",
    "                if word in stopwords:\n",
    "                    del tokens[index]\n",
    "                    \n",
    "                #delete number\n",
    "                if word.isdigit():\n",
    "                    del tokens[index]\n",
    "        \n",
    "        clean_corpus.append(\" \".join(tokens))  \n",
    "        token.append(list(dict.fromkeys(tokens)))\n",
    "        \n",
    "    corpus['clean_corpus'] = clean_corpus\n",
    "    corpus['terms'] = token\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function calculates term weighting\n",
    "def get_term_weighting_score(cleaning_result):\n",
    "    \n",
    "    #getting all the terms\n",
    "    terms = []\n",
    "    for index, sentence in enumerate(cleaning_result['terms']):\n",
    "        terms += [temp for temp in sentence if temp not in terms]\n",
    "    terms.sort()\n",
    "        \n",
    "    #getting frequency for every sentences\n",
    "    terms_frequency = pd.DataFrame()\n",
    "    for index, term in enumerate(cleaning_result['terms']):\n",
    "        frequency_each_sentence = []\n",
    "        for i, d in enumerate(terms):\n",
    "            temp = term.count(d)\n",
    "            frequency_each_sentence.append(temp)\n",
    "        terms_frequency[str(index+1)] = frequency_each_sentence\n",
    "        \n",
    "    terms_frequency['terms'] = terms\n",
    "    terms_frequency.set_index('terms', inplace= True)\n",
    "    \n",
    "    #getting df for every terms\n",
    "    df_idf = pd.DataFrame(terms_frequency.sum(axis=1), columns=['df_term'])\n",
    "    df_idf['terms'] = terms\n",
    "    df_idf.set_index('terms', inplace= True)\n",
    "    \n",
    "    #getting idf for every terms\n",
    "    N = len(terms_frequency.columns)\n",
    "    terms_idf = []\n",
    "    for i, d in df_idf.iterrows():\n",
    "        idf_score = math.log((N+1)/((df_idf['df_term'][i])), 10)\n",
    "        terms_idf.append(idf_score)\n",
    "    df_idf['idf_term'] = terms_idf\n",
    "    \n",
    "    return terms_frequency, df_idf   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "    np.random.seed(0)\n",
    "    def __init__(self, id, full_sentence, clean_sentence, tokens):\n",
    "        self.list_bm25 = {}\n",
    "        self.pagerank_score = random.random()\n",
    "        self.id = id\n",
    "        self.full_sentence = full_sentence\n",
    "        self.clean_sentence = clean_sentence\n",
    "        self.tokens = tokens\n",
    "        self.sentence_len = len(clean_sentence.split())\n",
    "        self.pagerank_score_new = 0\n",
    "        self.pagerank_before_maxiter = 0\n",
    "            \n",
    "    def calculate_bm25(self, raw_frequency, idf, doc, slen_ave):\n",
    "        k1 = 1.2\n",
    "        b = 0.75\n",
    "        total_bm25 = 0\n",
    "        for query in self.tokens:\n",
    "            tf = raw_frequency.at[str(query),str(doc.id)]\n",
    "            idff = idf.at[str(query),'idf_term']\n",
    "            temp = idff * ((k1+1) * tf) / (k1*( (1-b) + (b *(doc.sentence_len/slen_ave)) ) + tf)  \n",
    "            total_bm25 += temp\n",
    "        self.list_bm25[doc.id] = total_bm25\n",
    "        \n",
    "    def calculate_new_pagerank(self, doc):\n",
    "        d=0.85\n",
    "        sum_InVi = 0\n",
    "        for item in doc:\n",
    "            if self.id is not item.id:\n",
    "                Wji = self.list_bm25[item.id]\n",
    "                total_Wjk = sum(item.list_bm25.values())\n",
    "                sum_InVi += Wji/total_Wjk*item.pagerank_score\n",
    "        self.pagerank_score_new = (1-d)+(d*sum_InVi)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:    \n",
    "    def __init__(self, result_doc, raw_frequency, idf, cr):\n",
    "        self.raw_frequency = raw_frequency\n",
    "        self.idf = idf.drop(columns=['df_term'])\n",
    "        self.result_doc = result_doc\n",
    "        self.total_doc = len(result_doc['dokumen'])-1\n",
    "        self.slen_ave = 0\n",
    "        self.summarize = []\n",
    "        self.doc = []\n",
    "        self.outlier = []\n",
    "        self.compression_rate = cr\n",
    "        self.doc_utuh = []\n",
    "        \n",
    "        \n",
    "        #making object sentence\n",
    "#         list_pgrk = [0.400827866,0.863170087,0.389187762,0.924094751,0.157640608,\n",
    "#                      0.714980958,0.216858534,0.237221536,0.076112858,0.841401681]\n",
    "\n",
    "        for index, item in self.result_doc.iterrows():\n",
    "            self.doc.append(Sentence((index+1), item['dokumen'], item['clean_corpus'], \n",
    "                                     item['terms'])) \n",
    "            self.doc_utuh.append(Sentence((index+1), item['dokumen'], item['clean_corpus'], \n",
    "                                          item['terms'])) \n",
    "        \n",
    "        #calculate len average\n",
    "        temp_len_doc = 0\n",
    "        for item in self.doc:\n",
    "            temp_len_doc += item.sentence_len\n",
    "        self.slen_ave = temp_len_doc/len(self.doc)\n",
    "        \n",
    "        \n",
    "        #calculate bm25 for each object sentence\n",
    "        for item in self.doc:\n",
    "            for item2 in self.doc:\n",
    "                if item.id is not item2.id:\n",
    "                    item.calculate_bm25(raw_frequency= self.raw_frequency, \n",
    "                                        idf=self.idf, doc=item2, slen_ave=self.slen_ave)\n",
    "           \n",
    "#         self.doc_utuh = self.doc\n",
    "        ##CHECKING IF BM25 SCORE IS 0 (OUTLIER SENTENCE)\n",
    "        for index, item in enumerate(self.doc):\n",
    "            if sum(item.list_bm25.values()) <= 0:\n",
    "                self.outlier.append(self.doc.pop(index))\n",
    "                \n",
    "                \n",
    "#         calculate pagerank\n",
    "        for i in range(100): #reference: https://networkx.github.io/documentation/networkx-1.10/reference/generated/networkx.algorithms.link_analysis.pagerank_alg.pagerank.html  \n",
    "            for item in self.doc:\n",
    "                item.calculate_new_pagerank(self.doc)\n",
    "                if i==98:\n",
    "                    item.pagerank_before_maxiter = item.pagerank_score\n",
    "    #                 print(item.pagerank_before_maxiter)\n",
    "\n",
    "            #update pagerank score\n",
    "            for item in self.doc:\n",
    "                item.pagerank_score = item.pagerank_score_new\n",
    "        \n",
    "#         for item in self.doc:\n",
    "#             print(item.full_sentence,\"selisih pr : \",abs(item.pagerank_before_maxiter-item.pagerank_score) ,\n",
    "#                   \"pr sebelumnya : \",item.pagerank_before_maxiter , \"prnya : \", item.pagerank_score)\n",
    "            \n",
    "        #membuang title sebelum mengurutkan\n",
    "#         self.doc.pop(0)\n",
    "#         self.doc_utuh.pop(0)\n",
    "        \n",
    "\n",
    "        #getting the summarize        \n",
    "        sorted_doc = sorted(self.doc, key=lambda x: x.pagerank_score, reverse=True)        \n",
    "        top_pagerank = []\n",
    "        for item in range(math.ceil(self.total_doc*self.compression_rate)):\n",
    "            top_pagerank.append(sorted_doc[item])\n",
    "            \n",
    "#         for item in top_pagerank:\n",
    "#             print(item.full_sentence)\n",
    "        \n",
    "        sorted_sum = sorted(top_pagerank, key=lambda x: x.id)\n",
    "        \n",
    "        temp_summarize = [item.full_sentence for item in sorted_sum]\n",
    "        \n",
    "#         self.summarize = sorted_sum\n",
    "        \n",
    "        self.summarize = temp_summarize   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document(url_bbcnews):\n",
    "    \n",
    "    try:\n",
    "        page_news = urlopen(url_bbcnews)\n",
    "    except:\n",
    "        return \"error\", \"none\"\n",
    "    \n",
    "    beautysoup = BeautifulSoup(page_news, 'html.parser')\n",
    "    title =  beautysoup.find('h1', {\"class\": \"story-body__h1\"}).text\n",
    "    text_news = beautysoup.find('div', {\"class\": \"story-body__inner\"})\n",
    "\n",
    "    document = ''\n",
    "    for data in text_news.findAll('p'):\n",
    "        document = document + ' ' +  data.text\n",
    "    \n",
    "    return document, title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main sistem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL berita BBC Indonesia: https://www.bbc.com/indonesia/indonesia-50038237\n",
      "Jenis compression rate:\n",
      " 1. 5% \n",
      " 2. 10% \n",
      " 3. 20% \n",
      " 4. 30%\n",
      "Pilih jenis compression rate: 4\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Judul berita:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asap Palembang: Kabut selimuti ibu kota Sumatera Selatan, siswa sekolah diliburkan\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Hasil ringkasan:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sejumlah sekolah di Kota Palembang, Sumatera Selatan, memutuskan memulangkan siswa-siswa mereka lantaran kabut asap semakin tebal menyelimuti kota tersebut. \"Pagi ini kami memulangkan siswa karena melihat kabut asap yang tebal dan berdampak buruk terhadap siswa, oleh karenanya atas instruksi Kadiknas Kota Palembang melalui pesan WA Grup meminta siswa dipulangkan dan belajar di rumah masing-masing saja,\" jelas Siti kepada radio Elshinta. Akan tetapi, sebagaimana dipaparkan Kepala Dinas Pendidikan Sumatera Selatan, Widodo, kegiatan belajar mengajar di daerah yang tidak terdampak kabut asap tetap berlangsung. \"Daerah yang tidak terdampak kabut asap tetap normal tetap belajar, untuk daerah yang terkategori sedang tetap belajar namun jam masuk sekolah diundur dan kami himbau memakai masker, bagi daerah terkategori parah maka siswa diberikan tugas dengan memaksimalkan kelas daring,\" kata Widodo kepada Antara. \"Melalui pesan digital, Kepala Dinas Pendidikan Kota Palembang menginstruksikan kegiatan belajar mengajar di tingkat paud, TK, SD dan SMP negeri dan swasta diliburkan hingga batas yang belum ditentukan,\" sebut Agus dalam siaran pers. Kepala Seksi Observasi dan Informasi Stasiun Meteorologi SMB II Palembang, Bambang Beny Setiaji, mengatakan kabut tersebut bercampur asap kiriman dari wilayah Kabupaten Ogan Komering Ilir (OKI) yang berada sebelah tenggara Kota Palembang. Kabut asap di Kota Palembang semakin parah dalam sepekan terakhir akibat dampak kebakaran hutan dan lahan di sejumlah kabupaten. Berdasarkan data Badan Penanggulangan Bencana Daerah Provinsi Sumsel yang bersumber dari Satelit Lapan disebutkan jumlah titik panas pada Senin (14/10) mencapai 732 titik, dengan titik panas terbanyak di Kabupaten Ogan Komering Ilir yang berjumlah 437 titik. Kepala Bidang Kedaruratan Badan Penanggulangan Bencana Daerah Provinsi Sumsel, Ansori, mengatakan titik panas terbanyak terpantau di Kabupaten Ogan Komering Ilir sehingga fokus pemadaman difokuskan di wilayah tersebut. Sementara itu, aktivitas kapal bertonase di Sungai Musi, Kota Palembang, dihentikan akibat kabut asap pekat. \n"
     ]
    }
   ],
   "source": [
    "# document = get_document(\"https://www.bbc.com/indonesia/indonesia-50038237\")\n",
    "document2 = input(\"URL berita BBC Indonesia: \") \n",
    "\n",
    "print(\"Jenis compression rate:\\n 1. 5% \\n 2. 10% \\n 3. 20% \\n 4. 30%\")\n",
    "cr2 = input(\"Pilih jenis compression rate: \")\n",
    "\n",
    "\n",
    "if cr2 in [\"1\",\"2\",\"3\",\"4\"] :\n",
    "#     print(cr2)\n",
    "    stopword = open(\"../stopword_list_tala.txt\", \"r\")\n",
    "    stopwords = stopword.read().split(\"\\n\")\n",
    "    document3, title = get_document(document2)\n",
    "\n",
    "    if document3 == \"error\" or title == \"none\":\n",
    "        display(Markdown('**sorry, i cant access the url**'))\n",
    "    else:      \n",
    "#         document4 = title+\". \"+document3\n",
    "    \n",
    "        cleaning_result2 = get_clean_corpus(corpus=document3, stopwords=stopwords)\n",
    "        terms_frequency2, df_idf2  = get_term_weighting_score(cleaning_result=cleaning_result2)\n",
    "        \n",
    "        if cr2 == \"1\":\n",
    "            crate = 0.05\n",
    "        elif cr2 == \"2\":\n",
    "            crate = 0.1\n",
    "        elif cr2 == \"3\":\n",
    "            crate = 0.2\n",
    "        elif cr2 == \"4\":\n",
    "            crate = 0.3\n",
    "\n",
    "        percobaan2 = Graph(result_doc=cleaning_result2, raw_frequency=terms_frequency2, \n",
    "                              idf=df_idf2, cr=float(crate))\n",
    "        display(Markdown('**Judul berita:**'))\n",
    "        print(title)\n",
    "\n",
    "        summarize = \"\"\n",
    "        for ringkasan in percobaan2.summarize:\n",
    "            summarize += ringkasan + \" \"\n",
    "\n",
    "        display(Markdown('**Hasil ringkasan:**'))\n",
    "        print(summarize)\n",
    "else:\n",
    "    display(Markdown('**sorry, wrong input**'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
