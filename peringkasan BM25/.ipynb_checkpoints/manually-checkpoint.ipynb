{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# import StemmerFactory class\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function does cleaning, tokenize, remove stopwords, and stemming\n",
    "def get_clean_corpus(corpus, stopwords):\n",
    "    \n",
    "    #segmentasi\n",
    "    temp = sent_tokenize(corpus)\n",
    "    corpus = pd.DataFrame(temp, columns=['dokumen'])\n",
    "    \n",
    "    clean_corpus = []\n",
    "    token = []\n",
    "    for index, sentence in enumerate(corpus['dokumen']):\n",
    "        term = word_tokenize(corpus['dokumen'][index])\n",
    "        \n",
    "        #deleting url\n",
    "        deleted_url = [temp for temp in term if not re.match(r\"\\w+(?:(\\.(\\w+)\\.(\\w+)))|\\w+(?:(\\.(\\w+)))\", str(temp))]\n",
    "        \n",
    "        #deleting symbol\n",
    "        deleted_symbol = [re.sub(r\"[\\-\\+\\=\\:\\;\\\"\\\\\\@\\[\\]\\,_!;.':#$%^&*()<>?/\\|}{~:]\",\" \",str(temp)) for temp in deleted_url ]\n",
    "        \n",
    "        #stemming\n",
    "        stemmed_sentence = stemmer.stem(\" \".join(deleted_symbol))\n",
    "        \n",
    "        tokens = word_tokenize(stemmed_sentence)\n",
    "        \n",
    "        for i in range(len(tokens)):\n",
    "            for index, word in enumerate(tokens):\n",
    "                #delete stopwprds\n",
    "                if word in stopwords:\n",
    "                    del tokens[index]\n",
    "                    \n",
    "                #delete number\n",
    "                if word.isdigit():\n",
    "                    del tokens[index]\n",
    "        \n",
    "        clean_corpus.append(\" \".join(tokens))  \n",
    "        token.append(list(dict.fromkeys(tokens)))\n",
    "        \n",
    "    corpus['clean_corpus'] = clean_corpus\n",
    "    corpus['terms'] = token\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_document = open(\"bbc contoh beneran.txt\", \"r\")\n",
    "document= input_document.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = open(\"stopword_list_tala.txt\", \"r\")\n",
    "stopwords = stopword.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_result = get_clean_corpus(corpus=document, stopwords=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dokumen</th>\n",
       "      <th>clean_corpus</th>\n",
       "      <th>terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sejumlah sekolah di Kota Palembang, Sumatera S...</td>\n",
       "      <td>sekolah kota palembang sumatera selatan putus ...</td>\n",
       "      <td>[sekolah, kota, palembang, sumatera, selatan, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kepala SMP Negeri 7 Palembang, Siti Zubaida, m...</td>\n",
       "      <td>kepala smp negeri palembang siti zubaida putus...</td>\n",
       "      <td>[kepala, smp, negeri, palembang, siti, zubaida...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Pagi ini kami memulangkan siswa karena meliha...</td>\n",
       "      <td>pagi pulang siswa lihat kabut asap tebal dampa...</td>\n",
       "      <td>[pagi, pulang, siswa, lihat, kabut, asap, teba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hal ini diamini Kepala Dinas Pendidikan Kota P...</td>\n",
       "      <td>amin kepala dinas didik kota palembang ahmad z...</td>\n",
       "      <td>[amin, kepala, dinas, didik, kota, palembang, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Hari ini seluruh TK hingga SMP negeri dan swa...</td>\n",
       "      <td>tk smp negeri swasta derajat libur besok edar ...</td>\n",
       "      <td>[tk, smp, negeri, swasta, derajat, libur, beso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Menurutnya, kalau kualitas udara Palembang mas...</td>\n",
       "      <td>kualitas udara palembang buruk giat ajar ajar ...</td>\n",
       "      <td>[kualitas, udara, palembang, buruk, giat, ajar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Akan tetapi, sebagaimana dipaparkan Kepala Din...</td>\n",
       "      <td>papar kepala dinas didik sumatera selatan wido...</td>\n",
       "      <td>[papar, kepala, dinas, didik, sumatera, selata...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"Daerah yang tidak terdampak kabut asap tetap ...</td>\n",
       "      <td>daerah dampak kabut asap normal ajar daerah ka...</td>\n",
       "      <td>[daerah, dampak, kabut, asap, normal, ajar, ka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hal itu belakangan dibenarkan Agus Wibowo, sel...</td>\n",
       "      <td>agus wibowo kepala pusat data informasi humas ...</td>\n",
       "      <td>[agus, wibowo, kepala, pusat, data, informasi,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"Melalui pesan digital, Kepala Dinas Pendidika...</td>\n",
       "      <td>pesan digital kepala dinas didik kota palemban...</td>\n",
       "      <td>[pesan, digital, kepala, dinas, didik, kota, p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             dokumen  \\\n",
       "0  Sejumlah sekolah di Kota Palembang, Sumatera S...   \n",
       "1  Kepala SMP Negeri 7 Palembang, Siti Zubaida, m...   \n",
       "2  \"Pagi ini kami memulangkan siswa karena meliha...   \n",
       "3  Hal ini diamini Kepala Dinas Pendidikan Kota P...   \n",
       "4  \"Hari ini seluruh TK hingga SMP negeri dan swa...   \n",
       "5  Menurutnya, kalau kualitas udara Palembang mas...   \n",
       "6  Akan tetapi, sebagaimana dipaparkan Kepala Din...   \n",
       "7  \"Daerah yang tidak terdampak kabut asap tetap ...   \n",
       "8  Hal itu belakangan dibenarkan Agus Wibowo, sel...   \n",
       "9  \"Melalui pesan digital, Kepala Dinas Pendidika...   \n",
       "\n",
       "                                        clean_corpus  \\\n",
       "0  sekolah kota palembang sumatera selatan putus ...   \n",
       "1  kepala smp negeri palembang siti zubaida putus...   \n",
       "2  pagi pulang siswa lihat kabut asap tebal dampa...   \n",
       "3  amin kepala dinas didik kota palembang ahmad z...   \n",
       "4  tk smp negeri swasta derajat libur besok edar ...   \n",
       "5  kualitas udara palembang buruk giat ajar ajar ...   \n",
       "6  papar kepala dinas didik sumatera selatan wido...   \n",
       "7  daerah dampak kabut asap normal ajar daerah ka...   \n",
       "8  agus wibowo kepala pusat data informasi humas ...   \n",
       "9  pesan digital kepala dinas didik kota palemban...   \n",
       "\n",
       "                                               terms  \n",
       "0  [sekolah, kota, palembang, sumatera, selatan, ...  \n",
       "1  [kepala, smp, negeri, palembang, siti, zubaida...  \n",
       "2  [pagi, pulang, siswa, lihat, kabut, asap, teba...  \n",
       "3  [amin, kepala, dinas, didik, kota, palembang, ...  \n",
       "4  [tk, smp, negeri, swasta, derajat, libur, beso...  \n",
       "5  [kualitas, udara, palembang, buruk, giat, ajar...  \n",
       "6  [papar, kepala, dinas, didik, sumatera, selata...  \n",
       "7  [daerah, dampak, kabut, asap, normal, ajar, ka...  \n",
       "8  [agus, wibowo, kepala, pusat, data, informasi,...  \n",
       "9  [pesan, digital, kepala, dinas, didik, kota, p...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaning_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function calculates term weighting\n",
    "def get_term_weighting_score(cleaning_result):\n",
    "    \n",
    "    #getting all the terms\n",
    "    terms = []\n",
    "    for index, sentence in enumerate(cleaning_result['terms']):\n",
    "        terms += [temp for temp in sentence if temp not in terms]\n",
    "    terms.sort()\n",
    "        \n",
    "    #getting frequency for every sentences\n",
    "    terms_frequency = pd.DataFrame()\n",
    "    for index, term in enumerate(cleaning_result['terms']):\n",
    "        frequency_each_sentence = []\n",
    "        for i, d in enumerate(terms):\n",
    "            temp = term.count(d)\n",
    "            frequency_each_sentence.append(temp)\n",
    "        terms_frequency[str(index+1)] = frequency_each_sentence\n",
    "        \n",
    "    terms_frequency['terms'] = terms\n",
    "    terms_frequency.set_index('terms', inplace= True)\n",
    "    \n",
    "    #getting df for every terms\n",
    "    df_idf = pd.DataFrame(terms_frequency.sum(axis=1), columns=['df_term'])\n",
    "    df_idf['terms'] = terms\n",
    "    df_idf.set_index('terms', inplace= True)\n",
    "    \n",
    "    #getting idf for every terms\n",
    "    N = len(terms_frequency.columns)\n",
    "    terms_idf = []\n",
    "    for i, d in df_idf.iterrows():\n",
    "        idf_score = math.log((N+1)/((df_idf['df_term'][i]+1)), 10)+1\n",
    "        terms_idf.append(idf_score)\n",
    "    df_idf['idf_term'] = terms_idf\n",
    "    \n",
    "    return terms_frequency, df_idf   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_frequency, df_idf  = get_term_weighting_score(cleaning_result=cleaning_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# terms_frequency\n",
    "# df_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "    np.random.seed(0)\n",
    "    def __init__(self, id, full_sentence, clean_sentence, tokens, pagerank):\n",
    "        self.list_bm25 = {}\n",
    "#         self.pagerank_score = random.random()\n",
    "        self.pagerank_score = pagerank\n",
    "        self.id = id\n",
    "        self.full_sentence = full_sentence\n",
    "        self.clean_sentence = clean_sentence\n",
    "        self.tokens = tokens\n",
    "        self.sentence_len = len(clean_sentence.split())\n",
    "        self.pagerank_score_new = 0\n",
    "            \n",
    "    def calculate_bm25(self, raw_frequency, idf, doc, slen_ave):\n",
    "        k1 = 1.2\n",
    "        b = 0.75\n",
    "        total_bm25 = 0\n",
    "        for query in self.tokens:\n",
    "            tf = raw_frequency.at[str(query),str(doc.id)]\n",
    "            idff = idf.at[str(query),'idf_term']\n",
    "            temp = idff * ((k1+1) * tf) / (k1*( (1-b) + (b *(doc.sentence_len/slen_ave)) ) + tf)            \n",
    "            total_bm25 += temp\n",
    "        self.list_bm25[doc.id] = total_bm25\n",
    "        \n",
    "    def calculate_new_pagerank(self, doc):\n",
    "        d=0.85\n",
    "        sum_InVi = 0\n",
    "        for item in doc:\n",
    "            if self.id is not item.id:\n",
    "                Wji = self.list_bm25[item.id]\n",
    "                total_Wjk = sum(item.list_bm25.values())\n",
    "                sum_InVi += Wji/total_Wjk*item.pagerank_score\n",
    "        self.pagerank_score_new = (1-d)+(d*sum_InVi)\n",
    "#         print(self.pagerank_score_new)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:    \n",
    "    def __init__(self, result_doc, raw_frequency, idf):\n",
    "        self.raw_frequency = raw_frequency\n",
    "        self.idf = idf.drop(columns=['df_term'])\n",
    "        self.result_doc = result_doc\n",
    "        self.total_doc = len(result_doc['dokumen'])\n",
    "        self.slen_ave = 0\n",
    "        self.summarize = []\n",
    "        \n",
    "        \n",
    "        #making object sentence\n",
    "        list_pgrk = [0.400827866,0.863170087,0.389187762,0.924094751,0.157640608,0.714980958,0.216858534,0.237221536,0.076112858,0.841401681]\n",
    "\n",
    "        doc = []\n",
    "        for index, item in self.result_doc.iterrows():\n",
    "            doc.append(Sentence((index+1), item['dokumen'], item['clean_corpus'], item['terms'], pagerank=list_pgrk[index])) \n",
    "        \n",
    "        #calculate len average\n",
    "        temp_len_doc = 0\n",
    "        for item in doc:\n",
    "            temp_len_doc += item.sentence_len\n",
    "        self.slen_ave = temp_len_doc/len(doc)\n",
    "        \n",
    "        \n",
    "        #calculate bm25 for each object sentence\n",
    "        for item in doc:\n",
    "            for item2 in doc:\n",
    "                if item.id is not item2.id:\n",
    "                    item.calculate_bm25(raw_frequency= self.raw_frequency, idf=self.idf, doc=item2, slen_ave=self.slen_ave)\n",
    "                    \n",
    "        #calculate pagerank\n",
    "        for i in range(20):\n",
    "            for item in doc:\n",
    "                item.calculate_new_pagerank(doc)\n",
    "            \n",
    "            #update pagerank score\n",
    "            for item in doc:\n",
    "                item.pagerank_score = item.pagerank_score_new\n",
    "        \n",
    "\n",
    "        #getting the summarize        \n",
    "        sorted_doc = sorted(doc, key=lambda x: x.pagerank_score, reverse=True)        \n",
    "        top_pagerank = []\n",
    "        for item in range(math.ceil(self.total_doc*0.25)):\n",
    "            top_pagerank.append(sorted_doc[item])\n",
    "        \n",
    "        sorted_sum = sorted(top_pagerank, key=lambda x: x.id)\n",
    "        \n",
    "        temp_summarize = [item.full_sentence for item in sorted_sum]\n",
    "        \n",
    "        self.summarize = sorted_sum\n",
    "#         self.summarize = temp_summarize\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Kepala SMP Negeri 7 Palembang, Siti Zubaida, mengatakan keputusan pemulangan ditempuh sesuai dengan instruksi Dinas Pendidikan Kota Palembang.\n",
      "1.2186868189941311\n",
      "3\n",
      "\"Pagi ini kami memulangkan siswa karena melihat kabut asap yang tebal dan berdampak buruk terhadap siswa, oleh karenanya atas instruksi Kadiknas Kota Palembang melalui pesan WA Grup meminta siswa dipulangkan dan belajar di rumah masing-masing saja,\" jelas Siti kepada radio Elshinta.\n",
      "1.2882431241496164\n",
      "10\n",
      "\"Melalui pesan digital, Kepala Dinas Pendidikan Kota Palembang menginstruksikan kegiatan belajar mengajar di tingkat paud, TK, SD dan SMP negeri dan swasta diliburkan hingga batas yang belum ditentukan,\" sebut Agus dalam siaran pers.\n",
      "1.742006231091659\n"
     ]
    }
   ],
   "source": [
    "cobs = Graph(result_doc=cleaning_result, raw_frequency=terms_frequency, idf=df_idf)\n",
    "for item in cobs.summarize:\n",
    "    print(item.id)\n",
    "    print(item.full_sentence)\n",
    "    print(item.pagerank_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
