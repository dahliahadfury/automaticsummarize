{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# import StemmerFactory class\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function does cleaning, tokenize, remove stopwords, and stemming\n",
    "def get_clean_corpus(corpus, stopwords):\n",
    "    \n",
    "    #segmentasi\n",
    "    temp = sent_tokenize(corpus)\n",
    "    corpus = pd.DataFrame(temp, columns=['dokumen'])\n",
    "    \n",
    "    clean_corpus = []\n",
    "    token = []\n",
    "    for index, sentence in enumerate(corpus['dokumen']):\n",
    "        term = word_tokenize(corpus['dokumen'][index])\n",
    "        \n",
    "        #deleting url\n",
    "        deleted_url = [temp for temp in term if not \n",
    "                       re.match(r\"\\w+(?:(\\.(\\w+)\\.(\\w+)))|\\w+(?:(\\.(\\w+)))\", str(temp))]\n",
    "        \n",
    "        #deleting symbol\n",
    "        deleted_symbol = [re.sub(r\"[\\-\\+\\=\\:\\;\\\"\\\\\\@\\[\\]\\,_!;.':#$%^&*()<>?/\\|}{~:]\",\" \",\n",
    "                                 str(temp)) for temp in deleted_url ]\n",
    "        \n",
    "        #stemming\n",
    "        stemmed_sentence = stemmer.stem(\" \".join(deleted_symbol))\n",
    "        \n",
    "        tokens = word_tokenize(stemmed_sentence)\n",
    "        for i in range(len(tokens)):\n",
    "            for index, word in enumerate(tokens):\n",
    "                #delete stopwprds\n",
    "                if word in stopwords:\n",
    "                    del tokens[index]\n",
    "                    \n",
    "                #delete number\n",
    "                if word.isdigit():\n",
    "                    del tokens[index]\n",
    "        \n",
    "        clean_corpus.append(\" \".join(tokens))  \n",
    "        token.append(list(dict.fromkeys(tokens)))\n",
    "        \n",
    "    corpus['clean_corpus'] = clean_corpus\n",
    "    corpus['terms'] = token\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function calculates term weighting\n",
    "def get_term_weighting_score(cleaning_result):\n",
    "    \n",
    "    #getting all the terms\n",
    "    terms = []\n",
    "    for index, sentence in enumerate(cleaning_result['terms']):\n",
    "        terms += [temp for temp in sentence if temp not in terms]\n",
    "    terms.sort()\n",
    "        \n",
    "    #getting frequency for every sentences\n",
    "    terms_frequency = pd.DataFrame()\n",
    "    for index, term in enumerate(cleaning_result['terms']):\n",
    "        frequency_each_sentence = []\n",
    "        for i, d in enumerate(terms):\n",
    "            temp = term.count(d)\n",
    "            frequency_each_sentence.append(temp)\n",
    "        terms_frequency[str(index+1)] = frequency_each_sentence\n",
    "        \n",
    "    terms_frequency['terms'] = terms\n",
    "    terms_frequency.set_index('terms', inplace= True)\n",
    "    \n",
    "    #getting df for every terms\n",
    "    df_idf = pd.DataFrame(terms_frequency.sum(axis=1), columns=['df_term'])\n",
    "    df_idf['terms'] = terms\n",
    "    df_idf.set_index('terms', inplace= True)\n",
    "    \n",
    "    #getting idf for every terms\n",
    "    N = len(terms_frequency.columns)\n",
    "    terms_idf = []\n",
    "    for i, d in df_idf.iterrows():\n",
    "        idf_score = math.log((N+1)/((df_idf['df_term'][i])), 10)\n",
    "        terms_idf.append(idf_score)\n",
    "    df_idf['idf_term'] = terms_idf\n",
    "    \n",
    "    return terms_frequency, df_idf   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "    np.random.seed(0)\n",
    "    def __init__(self, id, full_sentence, clean_sentence, tokens):\n",
    "        self.list_bm25 = {}\n",
    "        self.pagerank_score = random.random()\n",
    "        self.id = id\n",
    "        self.full_sentence = full_sentence\n",
    "        self.clean_sentence = clean_sentence\n",
    "        self.tokens = tokens\n",
    "        self.sentence_len = len(clean_sentence.split())\n",
    "        self.pagerank_score_new = 0\n",
    "            \n",
    "    def calculate_bm25(self, raw_frequency, idf, doc, slen_ave):\n",
    "        k1 = 1.2\n",
    "        b = 0.75\n",
    "        total_bm25 = 0\n",
    "        for query in self.tokens:\n",
    "            tf = raw_frequency.at[str(query),str(doc.id)]\n",
    "            idff = idf.at[str(query),'idf_term']\n",
    "            temp = idff * ((k1+1) * tf) / (k1*( (1-b) + (b *(doc.sentence_len/slen_ave)) ) + tf)  \n",
    "            total_bm25 += temp\n",
    "        self.list_bm25[doc.id] = total_bm25\n",
    "        \n",
    "    def calculate_new_pagerank(self, doc):\n",
    "        d=0.85\n",
    "        sum_InVi = 0\n",
    "        for item in doc:\n",
    "            if self.id is not item.id:\n",
    "                Wji = self.list_bm25[item.id]\n",
    "                total_Wjk = sum(item.list_bm25.values())\n",
    "                sum_InVi += Wji/total_Wjk*item.pagerank_score\n",
    "        self.pagerank_score_new = (1-d)+(d*sum_InVi)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:    \n",
    "    def __init__(self, result_doc, raw_frequency, idf, cr):\n",
    "        self.raw_frequency = raw_frequency\n",
    "        self.idf = idf.drop(columns=['df_term'])\n",
    "        self.result_doc = result_doc\n",
    "        self.total_doc = len(result_doc['dokumen'])\n",
    "        self.slen_ave = 0\n",
    "        self.summarize = []\n",
    "        self.doc = []\n",
    "        self.outlier = []\n",
    "        self.compression_rate = cr\n",
    "        self.doc_utuh = []\n",
    "        \n",
    "        \n",
    "        #making object sentence\n",
    "#         list_pgrk = [0.400827866,0.863170087,0.389187762,0.924094751,0.157640608,\n",
    "#                      0.714980958,0.216858534,0.237221536,0.076112858,0.841401681]\n",
    "\n",
    "        for index, item in self.result_doc.iterrows():\n",
    "            self.doc.append(Sentence((index+1), item['dokumen'], item['clean_corpus'], \n",
    "                                     item['terms'])) \n",
    "            self.doc_utuh.append(Sentence((index+1), item['dokumen'], item['clean_corpus'], \n",
    "                                          item['terms'])) \n",
    "        \n",
    "        #calculate len average\n",
    "        temp_len_doc = 0\n",
    "        for item in self.doc:\n",
    "            temp_len_doc += item.sentence_len\n",
    "        self.slen_ave = temp_len_doc/len(self.doc)\n",
    "        \n",
    "        \n",
    "        #calculate bm25 for each object sentence\n",
    "        for item in self.doc:\n",
    "            for item2 in self.doc:\n",
    "                if item.id is not item2.id:\n",
    "                    item.calculate_bm25(raw_frequency= self.raw_frequency, \n",
    "                                        idf=self.idf, doc=item2, slen_ave=self.slen_ave)\n",
    "           \n",
    "#         self.doc_utuh = self.doc\n",
    "        ##CHECKING IF BM25 SCORE IS 0 (OUTLIER SENTENCE)\n",
    "        for index, item in enumerate(self.doc):\n",
    "            if sum(item.list_bm25.values()) <= 0:\n",
    "                self.outlier.append(self.doc.pop(index))\n",
    "                \n",
    "                \n",
    "        #calculate pagerank\n",
    "        for i in range(200):\n",
    "            for item in self.doc:\n",
    "                item.calculate_new_pagerank(self.doc)\n",
    "            \n",
    "            #update pagerank score\n",
    "            for item in self.doc:\n",
    "                item.pagerank_score = item.pagerank_score_new\n",
    "        \n",
    "\n",
    "        #getting the summarize        \n",
    "        sorted_doc = sorted(self.doc, key=lambda x: x.pagerank_score, reverse=True)        \n",
    "        top_pagerank = []\n",
    "        for item in range(math.ceil(self.total_doc*self.compression_rate)):\n",
    "            top_pagerank.append(sorted_doc[item])\n",
    "        \n",
    "        sorted_sum = sorted(top_pagerank, key=lambda x: x.id)\n",
    "        \n",
    "        temp_summarize = [item.full_sentence for item in sorted_sum]\n",
    "        \n",
    "#         self.summarize = sorted_sum\n",
    "        \n",
    "        self.summarize = temp_summarize   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sejumlah sekolah di Kota Palembang, Sumatera Selatan, memutuskan memulangkan siswa-siswa mereka lantaran kabut asap semakin tebal menyelimuti kota tersebut. pada Senin (14/10) pagi. Kepala SMP Negeri 7 Palembang, Siti Zubaida, mengatakan keputusan pemulangan ditempuh sesuai dengan instruksi Dinas Pendidikan Kota Palembang. \"Pagi ini kami memulangkan siswa karena melihat kabut asap yang tebal dan berdampak buruk terhadap siswa, oleh karenanya atas instruksi Kadiknas Kota Palembang melalui pesan WA Grup meminta siswa dipulangkan dan belajar di rumah masing-masing saja,\" jelas Siti kepada radio Elshinta. Hal ini diamini Kepala Dinas Pendidikan Kota Palembang, Ahmad Zulinto, yang menyampaikan surat edaran ke semua sekolah. \"Hari ini seluruh TK hingga SMP negeri dan swasta sederajat diliburkan, untuk besok dan seterusnya akan diberikan edaran lebih lanjut,\" kata Ahmad Zulinto kepada kantor berita Antara. Menurutnya, kalau kualitas udara Palembang masih buruk dalam beberapa hari ke depan, kegiatan belajar mengajar di tingkat TK sampai SMP akan tetap diliburkan.  Akan tetapi, sebagaimana dipaparkan Kepala Dinas Pendidikan Sumatera Selatan, Widodo, kegiatan belajar mengajar di daerah yang tidak terdampak kabut asap tetap berlangsung. \"Daerah yang tidak terdampak kabut asap tetap normal tetap belajar, untuk daerah yang terkategori sedang tetap belajar namun jam masuk sekolah diundur dan kami himbau memakai masker, bagi daerah terkategori parah maka siswa diberikan tugas dengan memaksimalkan kelas daring,\" kata Widodo kepada Antara. Hal itu belakangan dibenarkan Agus Wibowo, selaku Kepala Pusat Data, Informasi dan Humas Badan Nasional Penanggulangan Bencana (BNPB). \"Melalui pesan digital, Kepala Dinas Pendidikan Kota Palembang menginstruksikan kegiatan belajar mengajar di tingkat paud, TK, SD dan SMP negeri dan swasta diliburkan hingga batas yang belum ditentukan,\" sebut Agus dalam siaran pers. Sejumlah warga Palembang, Sumatera Selatan, mengeluhkan kabut asap pada Senin (14/10) tergolong parah. Bahkan jarak pandang hanya 10 meter.  Amelia, warga Kelurahan Bukit Sangkal, Kecamatan Kalidoni, mengatakan dirinya terkejut ketika akan ke luar rumah sekitar pukul 06.30 WIB untuk mengantar anaknya ke sekolah mendapati kabut sedemikian pekat.  \"Saya terkejut, kenapa gelap ini. Kemarin-kemarin ada kabut asap, tapi tidak separah hari ini,\" kata dia sebagaimana dikutip Antara. Keluhan atas kondisi ini juga diungkapkan Tina, seorang guru senam di sebuah tempat kebugaran. \"Saya selalu ke luar rumah pukul 06.00 WIB karena ada jadwal senam, sempat terkejut juga karena jarak pandang hanya 10 meter. Sangat terasa, apalagi saya pakai sepeda motor,\" kata dia.  Kepala Seksi Observasi dan Informasi Stasiun Meteorologi SMB II Palembang, Bambang Beny Setiaji, mengatakan kabut tersebut bercampur asap kiriman dari wilayah Kabupaten Ogan Komering Ilir (OKI) yang berada sebelah tenggara Kota Palembang.  Kabut asap di Kota Palembang semakin parah dalam sepekan terakhir akibat dampak kebakaran hutan dan lahan di sejumlah kabupaten.  Berdasarkan data Badan Penanggulangan Bencana Daerah Provinsi Sumsel yang bersumber dari Satelit Lapan disebutkan jumlah titik panas pada Senin (14/10) mencapai 732 titik, dengan titik panas terbanyak di Kabupaten Ogan Komering Ilir yang berjumlah 437 titik.  Sebelumnya, pada Jumat (11/10), titik panas berjumlah 417 titik.  Kepala Bidang Kedaruratan Badan Penanggulangan Bencana Daerah Provinsi Sumsel, Ansori, mengatakan titik panas terbanyak terpantau di Kabupaten Ogan Komering Ilir sehingga fokus pemadaman difokuskan di wilayah tersebut.  \"Kami selalu lakukan waterbombing (pemadaman dari udara), setiap hari mengerahkan lima unit helikopter. Kebakaran di OKI ini memang sulit dipadamkan karena terjadi di kawasan gambut, dan akses darat yang terbatas. belum lagi jika terbakar, asapnya mengarah ke Palembang,\" kata Ansori.  Sementara itu, aktivitas kapal bertonase di Sungai Musi, Kota Palembang, dihentikan akibat kabut asap pekat.  \"Para pandu kapal menunda gerakan kapal-kapal bertonase untuk sementara waktu,\" kata Kepala Seksi Lalu lintas pelayaran Kantor Kesyahbandaran dan Otoritas Pelabuhan (KSOP) Kelas II Palembang, Andriawan, kepada Antara.  Menurutnya KSOP masih memberlakukan aturan genap - ganjil untuk kapal bertonase masuk dan keluar dari wilayah Sungai Musi Palembang, namun batas waktu hanya pada rentang pukul 06.00 - 10.00 WIB.  Sementara kapal-kapal speadboat pembawa barang dari luar Palembang yang sandar di Dermaga 16 ilir Palembang juga menunda keberangkatannya akibat jarak pandang membahayakan.  \"Biasanya pukul 07.00 WIB kami sudah keluar dari Palembang, tapi sampai pukul 08.30 WIB belum bisa keluar karena bahaya sekali, apalagi jam 07.00 - 09.00 WIB itu ramai-ramainya kapal kecil,\" kata salah seorang nahkoda speadboat, Pardi.  Kabut asap tersebut, kata dia, mengakibatkan omzet pendapatan menurun 20%. \"Penumpang tidak ada yang mau naik karena mereka ada yang takut, ya jadinya kurang penumpang,\" tambahnya. \n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# specify the url\n",
    "url_bbcnews = \"https://www.bbc.com/indonesia/indonesia-50038237\"\n",
    "\n",
    "# Connect to the website and return the html to the variable ‘page’\n",
    "try:\n",
    "    page_news = urlopen(url_bbcnews)\n",
    "except:\n",
    "    print(\"Error opening the URL\")\n",
    "\n",
    "# parse the html using beautiful soup and store in variable `soup`\n",
    "beautysoup = BeautifulSoup(page_news, 'html.parser')\n",
    "\n",
    "# Take out the <div> of name and get its value\n",
    "text_news = beautysoup.find('div', {\"class\": \"story-body__inner\"})\n",
    "\n",
    "document = ''\n",
    "for i in text_news.findAll('p'):\n",
    "    document = document + ' ' +  i.text\n",
    "print(document)\n",
    "\n",
    "# Saving the scraped text\n",
    "# with open('scraped_text.txt', 'w') as file:\n",
    "#     file.write(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cr :  0.05\n",
      "\"Pagi ini kami memulangkan siswa karena melihat kabut asap yang tebal dan berdampak buruk terhadap siswa, oleh karenanya atas instruksi Kadiknas Kota Palembang melalui pesan WA Grup meminta siswa dipulangkan dan belajar di rumah masing-masing saja,\" jelas Siti kepada radio Elshinta.\n",
      "\"Melalui pesan digital, Kepala Dinas Pendidikan Kota Palembang menginstruksikan kegiatan belajar mengajar di tingkat paud, TK, SD dan SMP negeri dan swasta diliburkan hingga batas yang belum ditentukan,\" sebut Agus dalam siaran pers.\n",
      "cr :  0.1\n",
      "\"Pagi ini kami memulangkan siswa karena melihat kabut asap yang tebal dan berdampak buruk terhadap siswa, oleh karenanya atas instruksi Kadiknas Kota Palembang melalui pesan WA Grup meminta siswa dipulangkan dan belajar di rumah masing-masing saja,\" jelas Siti kepada radio Elshinta.\n",
      "\"Daerah yang tidak terdampak kabut asap tetap normal tetap belajar, untuk daerah yang terkategori sedang tetap belajar namun jam masuk sekolah diundur dan kami himbau memakai masker, bagi daerah terkategori parah maka siswa diberikan tugas dengan memaksimalkan kelas daring,\" kata Widodo kepada Antara.\n",
      "\"Melalui pesan digital, Kepala Dinas Pendidikan Kota Palembang menginstruksikan kegiatan belajar mengajar di tingkat paud, TK, SD dan SMP negeri dan swasta diliburkan hingga batas yang belum ditentukan,\" sebut Agus dalam siaran pers.\n",
      "Kepala Seksi Observasi dan Informasi Stasiun Meteorologi SMB II Palembang, Bambang Beny Setiaji, mengatakan kabut tersebut bercampur asap kiriman dari wilayah Kabupaten Ogan Komering Ilir (OKI) yang berada sebelah tenggara Kota Palembang.\n",
      "cr :  0.2\n",
      "\"Pagi ini kami memulangkan siswa karena melihat kabut asap yang tebal dan berdampak buruk terhadap siswa, oleh karenanya atas instruksi Kadiknas Kota Palembang melalui pesan WA Grup meminta siswa dipulangkan dan belajar di rumah masing-masing saja,\" jelas Siti kepada radio Elshinta.\n",
      "Akan tetapi, sebagaimana dipaparkan Kepala Dinas Pendidikan Sumatera Selatan, Widodo, kegiatan belajar mengajar di daerah yang tidak terdampak kabut asap tetap berlangsung.\n",
      "\"Daerah yang tidak terdampak kabut asap tetap normal tetap belajar, untuk daerah yang terkategori sedang tetap belajar namun jam masuk sekolah diundur dan kami himbau memakai masker, bagi daerah terkategori parah maka siswa diberikan tugas dengan memaksimalkan kelas daring,\" kata Widodo kepada Antara.\n",
      "\"Melalui pesan digital, Kepala Dinas Pendidikan Kota Palembang menginstruksikan kegiatan belajar mengajar di tingkat paud, TK, SD dan SMP negeri dan swasta diliburkan hingga batas yang belum ditentukan,\" sebut Agus dalam siaran pers.\n",
      "Kepala Seksi Observasi dan Informasi Stasiun Meteorologi SMB II Palembang, Bambang Beny Setiaji, mengatakan kabut tersebut bercampur asap kiriman dari wilayah Kabupaten Ogan Komering Ilir (OKI) yang berada sebelah tenggara Kota Palembang.\n",
      "Kabut asap di Kota Palembang semakin parah dalam sepekan terakhir akibat dampak kebakaran hutan dan lahan di sejumlah kabupaten.\n",
      "Kepala Bidang Kedaruratan Badan Penanggulangan Bencana Daerah Provinsi Sumsel, Ansori, mengatakan titik panas terbanyak terpantau di Kabupaten Ogan Komering Ilir sehingga fokus pemadaman difokuskan di wilayah tersebut.\n",
      "cr :  0.3\n",
      " Sejumlah sekolah di Kota Palembang, Sumatera Selatan, memutuskan memulangkan siswa-siswa mereka lantaran kabut asap semakin tebal menyelimuti kota tersebut.\n",
      "\"Pagi ini kami memulangkan siswa karena melihat kabut asap yang tebal dan berdampak buruk terhadap siswa, oleh karenanya atas instruksi Kadiknas Kota Palembang melalui pesan WA Grup meminta siswa dipulangkan dan belajar di rumah masing-masing saja,\" jelas Siti kepada radio Elshinta.\n",
      "Akan tetapi, sebagaimana dipaparkan Kepala Dinas Pendidikan Sumatera Selatan, Widodo, kegiatan belajar mengajar di daerah yang tidak terdampak kabut asap tetap berlangsung.\n",
      "\"Daerah yang tidak terdampak kabut asap tetap normal tetap belajar, untuk daerah yang terkategori sedang tetap belajar namun jam masuk sekolah diundur dan kami himbau memakai masker, bagi daerah terkategori parah maka siswa diberikan tugas dengan memaksimalkan kelas daring,\" kata Widodo kepada Antara.\n",
      "\"Melalui pesan digital, Kepala Dinas Pendidikan Kota Palembang menginstruksikan kegiatan belajar mengajar di tingkat paud, TK, SD dan SMP negeri dan swasta diliburkan hingga batas yang belum ditentukan,\" sebut Agus dalam siaran pers.\n",
      "Sejumlah warga Palembang, Sumatera Selatan, mengeluhkan kabut asap pada Senin (14/10) tergolong parah.\n",
      "Kepala Seksi Observasi dan Informasi Stasiun Meteorologi SMB II Palembang, Bambang Beny Setiaji, mengatakan kabut tersebut bercampur asap kiriman dari wilayah Kabupaten Ogan Komering Ilir (OKI) yang berada sebelah tenggara Kota Palembang.\n",
      "Kabut asap di Kota Palembang semakin parah dalam sepekan terakhir akibat dampak kebakaran hutan dan lahan di sejumlah kabupaten.\n",
      "Berdasarkan data Badan Penanggulangan Bencana Daerah Provinsi Sumsel yang bersumber dari Satelit Lapan disebutkan jumlah titik panas pada Senin (14/10) mencapai 732 titik, dengan titik panas terbanyak di Kabupaten Ogan Komering Ilir yang berjumlah 437 titik.\n",
      "Kepala Bidang Kedaruratan Badan Penanggulangan Bencana Daerah Provinsi Sumsel, Ansori, mengatakan titik panas terbanyak terpantau di Kabupaten Ogan Komering Ilir sehingga fokus pemadaman difokuskan di wilayah tersebut.\n",
      "Sementara itu, aktivitas kapal bertonase di Sungai Musi, Kota Palembang, dihentikan akibat kabut asap pekat.\n"
     ]
    }
   ],
   "source": [
    "# input_document = open(\"bbc contoh beneran.txt\", \"r\")\n",
    "# document2= document.readline()\n",
    "stopword = open(\"stopword_list_tala.txt\", \"r\")\n",
    "stopwords = stopword.read().split(\"\\n\")\n",
    "cleaning_result = get_clean_corpus(corpus=document, stopwords=stopwords)\n",
    "terms_frequency, df_idf  = get_term_weighting_score(cleaning_result=cleaning_result)\n",
    "cr = [0.05, 0.10, 0.20, 0.30]\n",
    "for crr in cr:\n",
    "    print(\"cr : \", crr)\n",
    "    percobaan = Graph(result_doc=cleaning_result, raw_frequency=terms_frequency, \n",
    "                      idf=df_idf, cr=crr)\n",
    "    for ringkasan in percobaan.summarize:\n",
    "        print(ringkasan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_document1 = open(\"dokumen uji/dok 1.txt\", \"r\")\n",
    "# document1= input_document1.readline()\n",
    "# cleaning_result1 = get_clean_corpus(corpus=document1, stopwords=stopwords)\n",
    "# terms_frequency1, df_idf1  = get_term_weighting_score(cleaning_result=cleaning_result1)\n",
    "# # cr = [0.05, 0.10, 0.20, 0.30]\n",
    "\n",
    "# hasil_dok1 = pd.DataFrame()\n",
    "# hasil_dok1[\"dokumen\"] =  cleaning_result1[\"dokumen\"]\n",
    "# for crr in cr:\n",
    "#     tempppppp=[]\n",
    "#     print(\"cr : \", crr)\n",
    "#     percobaan1 = Graph(result_doc=cleaning_result1, raw_frequency=terms_frequency1, \n",
    "#                        idf=df_idf1, cr=crr)\n",
    "    \n",
    "#     for index, itemmm in enumerate(percobaan1.doc_utuh):\n",
    "#         if itemmm.full_sentence in percobaan1.summarize:\n",
    "#             tempppppp.append(\"yes\")\n",
    "#         else:\n",
    "#             tempppppp.append(\"no\")\n",
    "#     hasil_dok1[str(crr)] = tempppppp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hasil_dok1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_document2 = open(\"dokumen uji/dok 2.txt\", \"r\")\n",
    "# document2= input_document2.readline()\n",
    "# cleaning_result2 = get_clean_corpus(corpus=document2, stopwords=stopwords)\n",
    "# terms_frequency2, df_idf2  = get_term_weighting_score(cleaning_result=cleaning_result2)\n",
    "# # cr = [0.05, 0.10, 0.20, 0.30]\n",
    "\n",
    "# hasil_dok2 = pd.DataFrame()\n",
    "# hasil_dok2[\"dokumen\"] =  cleaning_result2[\"dokumen\"]\n",
    "# for crr in cr:\n",
    "#     tempppppp = []\n",
    "#     print(\"cr : \", crr)\n",
    "#     percobaan2 = Graph(result_doc=cleaning_result2, raw_frequency=terms_frequency2, \n",
    "#                        idf=df_idf2, cr=crr)\n",
    "    \n",
    "#     for index, itemmm in enumerate(percobaan2.doc_utuh):\n",
    "#         if itemmm.full_sentence in percobaan2.summarize:\n",
    "#             tempppppp.append(\"yes\")\n",
    "#         else:\n",
    "#             tempppppp.append(\"no\")\n",
    "#     hasil_dok2[str(crr)] = tempppppp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hasil_dok2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_document3 = open(\"dokumen uji/dok 3.txt\", \"r\")\n",
    "# document3= input_document3.readline()\n",
    "# cleaning_result3 = get_clean_corpus(corpus=document3, stopwords=stopwords)\n",
    "# terms_frequency3, df_idf3  = get_term_weighting_score(cleaning_result=cleaning_result3)\n",
    "# # cr = [0.05, 0.10, 0.20, 0.30]\n",
    "\n",
    "# hasil_dok3 = pd.DataFrame()\n",
    "# hasil_dok3[\"dokumen\"] =  cleaning_result3[\"dokumen\"]\n",
    "# for crr in cr:\n",
    "#     tempppppp = []\n",
    "#     print(\"cr : \", crr)\n",
    "#     percobaan3 = Graph(result_doc=cleaning_result3, raw_frequency=terms_frequency3, \n",
    "#                        idf=df_idf3, cr=crr)\n",
    "    \n",
    "#     for index, itemmm in enumerate(percobaan3.doc_utuh):\n",
    "#         if itemmm.full_sentence in percobaan3.summarize:\n",
    "#             tempppppp.append(\"yes\")\n",
    "#         else:\n",
    "#             tempppppp.append(\"no\")\n",
    "#     hasil_dok3[str(crr)] = tempppppp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hasil_dok3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_document4 = open(\"dokumen uji/dok 4.txt\", \"r\")\n",
    "# document4= input_document4.readline()\n",
    "# cleaning_result4 = get_clean_corpus(corpus=document4, stopwords=stopwords)\n",
    "# terms_frequency4, df_idf4  = get_term_weighting_score(cleaning_result=cleaning_result4)\n",
    "# # cr = [0.05, 0.10, 0.20, 0.30]\n",
    "\n",
    "# hasil_dok4 = pd.DataFrame()\n",
    "# hasil_dok4[\"dokumen\"] =  cleaning_result4[\"dokumen\"]\n",
    "# for crr in cr:\n",
    "#     tempppppp = []\n",
    "#     print(\"cr : \", crr)\n",
    "#     percobaan4 = Graph(result_doc=cleaning_result4, raw_frequency=terms_frequency4, \n",
    "#                        idf=df_idf4, cr=crr)\n",
    "    \n",
    "#     for index, itemmm in enumerate(percobaan4.doc_utuh):\n",
    "#         if itemmm.full_sentence in percobaan4.summarize:\n",
    "#             tempppppp.append(\"yes\")\n",
    "#         else:\n",
    "#             tempppppp.append(\"no\")\n",
    "#     hasil_dok4[str(crr)] = tempppppp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hasil_dok4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_document5 = open(\"dokumen uji/dok 5.txt\", \"r\")\n",
    "# document5= input_document5.readline()\n",
    "# cleaning_result5 = get_clean_corpus(corpus=document5, stopwords=stopwords)\n",
    "# terms_frequency5, df_idf5  = get_term_weighting_score(cleaning_result=cleaning_result5)\n",
    "# # cr = [0.05, 0.10, 0.20, 0.30]\n",
    "# hasil_dok5 = pd.DataFrame()\n",
    "# hasil_dok5[\"dokumen\"] =  cleaning_result5[\"dokumen\"]\n",
    "# for crr in cr:\n",
    "#     tempppppp = []\n",
    "#     print(\"cr : \", crr)\n",
    "#     percobaan5 = Graph(result_doc=cleaning_result5, raw_frequency=terms_frequency5, \n",
    "#                        idf=df_idf5, cr=crr)\n",
    "    \n",
    "#     for index, itemmm in enumerate(percobaan5.doc_utuh):\n",
    "#         if itemmm.full_sentence in percobaan5.summarize:\n",
    "#             tempppppp.append(\"yes\")\n",
    "#         else:\n",
    "#             tempppppp.append(\"no\")\n",
    "#     hasil_dok5[str(crr)] = tempppppp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hasil_dok5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_document6 = open(\"dokumen uji/dok 6.txt\", \"r\")\n",
    "# document6= input_document6.readline()\n",
    "# cleaning_result6 = get_clean_corpus(corpus=document6, stopwords=stopwords)\n",
    "# terms_frequency6, df_idf6  = get_term_weighting_score(cleaning_result=cleaning_result6)\n",
    "# # cr = [0.05, 0.10, 0.20, 0.30]\n",
    "\n",
    "# hasil_dok6 = pd.DataFrame()\n",
    "# hasil_dok6[\"dokumen\"] =  cleaning_result6[\"dokumen\"]\n",
    "# for crr in cr:\n",
    "#     tempppppp = []\n",
    "#     print(\"cr : \", crr)\n",
    "#     percobaan6 = Graph(result_doc=cleaning_result6, raw_frequency=terms_frequency6, \n",
    "#                        idf=df_idf6, cr=crr)\n",
    "    \n",
    "#     for index, itemmm in enumerate(percobaan6.doc_utuh):\n",
    "#         if itemmm.full_sentence in percobaan6.summarize:\n",
    "#             tempppppp.append(\"yes\")\n",
    "#         else:\n",
    "#             tempppppp.append(\"no\")\n",
    "#     hasil_dok6[str(crr)] = tempppppp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hasil_dok6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_document7 = open(\"dokumen uji/dok 7.txt\", \"r\")\n",
    "# document7= input_document7.readline()\n",
    "# cleaning_result7 = get_clean_corpus(corpus=document7, stopwords=stopwords)\n",
    "# terms_frequency7, df_idf7  = get_term_weighting_score(cleaning_result=cleaning_result7)\n",
    "# # cr = [0.05, 0.10, 0.20, 0.30]\n",
    "\n",
    "# hasil_dok7 = pd.DataFrame()\n",
    "# hasil_dok7[\"dokumen\"] =  cleaning_result7[\"dokumen\"]\n",
    "# for crr in cr:\n",
    "#     tempppppp = []\n",
    "#     print(\"cr : \", crr)\n",
    "#     percobaan7 = Graph(result_doc=cleaning_result7, raw_frequency=terms_frequency7, idf=df_idf7, cr=crr)\n",
    "    \n",
    "#     for index, itemmm in enumerate(percobaan7.doc_utuh):\n",
    "#         if itemmm.full_sentence in percobaan7.summarize:\n",
    "#             tempppppp.append(\"yes\")\n",
    "#         else:\n",
    "#             tempppppp.append(\"no\")\n",
    "#     hasil_dok7[str(crr)] = tempppppp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hasil_dok7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_document8 = open(\"dokumen uji/dok 8.txt\", \"r\")\n",
    "# document8= input_document8.readline()\n",
    "# cleaning_result8 = get_clean_corpus(corpus=document8, stopwords=stopwords)\n",
    "# terms_frequency8, df_idf8  = get_term_weighting_score(cleaning_result=cleaning_result8)\n",
    "# # cr = [0.05, 0.10, 0.20, 0.30]\n",
    "\n",
    "# hasil_dok8 = pd.DataFrame()\n",
    "# hasil_dok8[\"dokumen\"] =  cleaning_result8[\"dokumen\"]\n",
    "# for crr in cr:\n",
    "#     tempppppp = []\n",
    "#     print(\"cr : \", crr)\n",
    "#     percobaan8 = Graph(result_doc=cleaning_result8, raw_frequency=terms_frequency8, \n",
    "#                        idf=df_idf8, cr=crr)\n",
    "    \n",
    "\n",
    "#     for index, itemmm in enumerate(percobaan8.doc_utuh):\n",
    "#         if itemmm.full_sentence in percobaan8.summarize:\n",
    "#             tempppppp.append(\"yes\")\n",
    "#         else:\n",
    "#             tempppppp.append(\"no\")\n",
    "#     hasil_dok8[str(crr)] = tempppppp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hasil_dok8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_document9 = open(\"dokumen uji/dok 9.txt\", \"r\")\n",
    "# document9= input_document9.readline()\n",
    "# cleaning_result9 = get_clean_corpus(corpus=document9, stopwords=stopwords)\n",
    "# terms_frequency9, df_idf9  = get_term_weighting_score(cleaning_result=cleaning_result9)\n",
    "# # cr = [0.05, 0.10, 0.20, 0.30]\n",
    "\n",
    "\n",
    "# hasil_dok9 = pd.DataFrame()\n",
    "# hasil_dok9[\"dokumen\"] =  cleaning_result9[\"dokumen\"]\n",
    "# for crr in cr:\n",
    "#     tempppppp = []\n",
    "#     print(\"cr : \", crr)\n",
    "#     percobaan9 = Graph(result_doc=cleaning_result9, \n",
    "#                         raw_frequency=terms_frequency9, idf=df_idf9, cr=crr)\n",
    "    \n",
    "# #     for ringkasan in percobaan10.summarize:\n",
    "# #         print(ringkasan)\n",
    "    \n",
    "#     for index, itemmm in enumerate(percobaan9.doc_utuh):\n",
    "#         if itemmm.full_sentence in percobaan9.summarize:\n",
    "#             tempppppp.append(\"yes\")\n",
    "#         else:\n",
    "#             tempppppp.append(\"no\")\n",
    "#     hasil_dok9[str(crr)] = tempppppp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hasil_dok9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_document10 = open(\"dokumen uji/dok 10.txt\", \"r\")\n",
    "# document10= input_document10.readline()\n",
    "# cleaning_result10 = get_clean_corpus(corpus=document10, stopwords=stopwords)\n",
    "# terms_frequency10, df_idf10  = get_term_weighting_score(cleaning_result=cleaning_result10)\n",
    "# # cr = [0.05, 0.10, 0.20, 0.30]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# hasil_dok10 = pd.DataFrame()\n",
    "# hasil_dok10[\"dokumen\"] =  cleaning_result10[\"dokumen\"]\n",
    "# for crr in cr:\n",
    "#     tempppppp = []\n",
    "#     print(\"cr : \", crr)\n",
    "#     percobaan10 = Graph(result_doc=cleaning_result10, \n",
    "#                         raw_frequency=terms_frequency10, idf=df_idf10, cr=crr)\n",
    "    \n",
    "#     for ringkasan in percobaan10.summarize:\n",
    "#         print(ringkasan)\n",
    "    \n",
    "#     for index, itemmm in enumerate(percobaan10.doc_utuh):\n",
    "#         if itemmm.full_sentence in percobaan10.summarize:\n",
    "#             tempppppp.append(\"yes\")\n",
    "#         else:\n",
    "#             tempppppp.append(\"no\")\n",
    "#     hasil_dok10[str(crr)] = tempppppp\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hasil_dok10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for index, itemmm in enumerate(percobaan10.doc_utuh):\n",
    "# #     print(index+1)\n",
    "#     print(itemmm.full_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hasil_dok1.to_csv(\"hasil sistem/hasil_sistem_dok1.csv\",index=False)\n",
    "# hasil_dok2.to_csv(\"hasil sistem/hasil_sistem_dok2.csv\",index=False)\n",
    "# hasil_dok3.to_csv(\"hasil sistem/hasil_sistem_dok3.csv\",index=False)\n",
    "# hasil_dok4.to_csv(\"hasil sistem/hasil_sistem_dok4.csv\",index=False)\n",
    "# hasil_dok5.to_csv(\"hasil sistem/hasil_sistem_dok5.csv\",index=False)\n",
    "# hasil_dok6.to_csv(\"hasil sistem/hasil_sistem_dok6.csv\",index=False)\n",
    "# hasil_dok7.to_csv(\"hasil sistem/hasil_sistem_dok7.csv\",index=False)\n",
    "# hasil_dok8.to_csv(\"hasil sistem/hasil_sistem_dok8.csv\",index=False)\n",
    "# hasil_dok9.to_csv(\"hasil sistem/hasil_sistem_dok9.csv\",index=False)\n",
    "# hasil_dok10.to_csv(\"hasil sistem/hasil_sistem_dok10.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hasil_dok10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
