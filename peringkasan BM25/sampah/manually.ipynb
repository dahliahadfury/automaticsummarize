{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# import StemmerFactory class\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function does cleaning, tokenize, remove stopwords, and stemming\n",
    "def get_clean_corpus(corpus, stopwords):\n",
    "    \n",
    "    #segmentasi\n",
    "    temp = sent_tokenize(corpus)\n",
    "    corpus = pd.DataFrame(temp, columns=['dokumen'])\n",
    "    \n",
    "    clean_corpus = []\n",
    "    token = []\n",
    "    for index, sentence in enumerate(corpus['dokumen']):\n",
    "        term = word_tokenize(corpus['dokumen'][index])\n",
    "        \n",
    "        #deleting url\n",
    "        deleted_url = [temp for temp in term if not re.match(r\"\\w+(?:(\\.(\\w+)\\.(\\w+)))|\\w+(?:(\\.(\\w+)))\", str(temp))]\n",
    "        \n",
    "        #deleting symbol\n",
    "        deleted_symbol = [re.sub(r\"[\\-\\+\\=\\:\\;\\\"\\\\\\@\\[\\]\\,_!;.':#$%^&*()<>?/\\|}{~:]\",\" \",str(temp)) for temp in deleted_url ]\n",
    "        \n",
    "#         print(\" \".join(deleted_symbol))\n",
    "        #stemming\n",
    "        stemmed_sentence = stemmer.stem(\" \".join(deleted_symbol))\n",
    "#         print(stemmed_sentence)\n",
    "        \n",
    "        tokens = word_tokenize(stemmed_sentence)\n",
    "#         print(tokens)\n",
    "        for i in range(len(tokens)):\n",
    "            for index, word in enumerate(tokens):\n",
    "                #delete stopwprds\n",
    "                if word in stopwords:\n",
    "                    del tokens[index]\n",
    "                    \n",
    "                #delete number\n",
    "                if word.isdigit():\n",
    "                    del tokens[index]\n",
    "        \n",
    "        clean_corpus.append(\" \".join(tokens))  \n",
    "        token.append(list(dict.fromkeys(tokens)))\n",
    "        \n",
    "    corpus['clean_corpus'] = clean_corpus\n",
    "    corpus['terms'] = token\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function calculates term weighting\n",
    "def get_term_weighting_score(cleaning_result):\n",
    "    \n",
    "    #getting all the terms\n",
    "    terms = []\n",
    "    for index, sentence in enumerate(cleaning_result['terms']):\n",
    "        terms += [temp for temp in sentence if temp not in terms]\n",
    "    terms.sort()\n",
    "        \n",
    "    #getting frequency for every sentences\n",
    "    terms_frequency = pd.DataFrame()\n",
    "    for index, term in enumerate(cleaning_result['terms']):\n",
    "        frequency_each_sentence = []\n",
    "        for i, d in enumerate(terms):\n",
    "            temp = term.count(d)\n",
    "            frequency_each_sentence.append(temp)\n",
    "        terms_frequency[str(index+1)] = frequency_each_sentence\n",
    "        \n",
    "    terms_frequency['terms'] = terms\n",
    "    terms_frequency.set_index('terms', inplace= True)\n",
    "    \n",
    "    #getting df for every terms\n",
    "    df_idf = pd.DataFrame(terms_frequency.sum(axis=1), columns=['df_term'])\n",
    "    df_idf['terms'] = terms\n",
    "    df_idf.set_index('terms', inplace= True)\n",
    "    \n",
    "    #getting idf for every terms\n",
    "    N = len(terms_frequency.columns)\n",
    "    terms_idf = []\n",
    "    for i, d in df_idf.iterrows():\n",
    "        idf_score = math.log((N+1)/((df_idf['df_term'][i])), 10)\n",
    "        terms_idf.append(idf_score)\n",
    "    df_idf['idf_term'] = terms_idf\n",
    "    \n",
    "    return terms_frequency, df_idf   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# terms_frequency\n",
    "# df_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "    np.random.seed(0)\n",
    "    def __init__(self, id, full_sentence, clean_sentence, tokens):\n",
    "        self.list_bm25 = {}\n",
    "        self.pagerank_score = random.random()\n",
    "#         self.pagerank_score = pagerank\n",
    "        self.id = id\n",
    "        self.full_sentence = full_sentence\n",
    "        self.clean_sentence = clean_sentence\n",
    "        self.tokens = tokens\n",
    "        self.sentence_len = len(clean_sentence.split())\n",
    "        self.pagerank_score_new = 0\n",
    "            \n",
    "    def calculate_bm25(self, raw_frequency, idf, doc, slen_ave):\n",
    "        k1 = 1.2\n",
    "        b = 0.75\n",
    "        total_bm25 = 0\n",
    "        for query in self.tokens:\n",
    "            tf = raw_frequency.at[str(query),str(doc.id)]\n",
    "            idff = idf.at[str(query),'idf_term']\n",
    "            temp = idff * ((k1+1) * tf) / (k1*( (1-b) + (b *(doc.sentence_len/slen_ave)) ) + tf)            \n",
    "            total_bm25 += temp\n",
    "        self.list_bm25[doc.id] = total_bm25\n",
    "        \n",
    "    def calculate_new_pagerank(self, doc):\n",
    "        d=0.85\n",
    "        sum_InVi = 0\n",
    "        for item in doc:\n",
    "            if self.id is not item.id:\n",
    "                Wji = self.list_bm25[item.id]\n",
    "                total_Wjk = sum(item.list_bm25.values())\n",
    "                sum_InVi += Wji/total_Wjk*item.pagerank_score\n",
    "        self.pagerank_score_new = (1-d)+(d*sum_InVi)\n",
    "#         print(self.pagerank_score_new)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:    \n",
    "    def __init__(self, result_doc, raw_frequency, idf, cr):\n",
    "        self.raw_frequency = raw_frequency\n",
    "        self.idf = idf.drop(columns=['df_term'])\n",
    "        self.result_doc = result_doc\n",
    "        self.total_doc = len(result_doc['dokumen'])\n",
    "        self.slen_ave = 0\n",
    "        self.summarize = []\n",
    "        self.doc = []\n",
    "        self.outlier = []\n",
    "        self.compression_rate = cr\n",
    "        \n",
    "        \n",
    "        #making object sentence\n",
    "        list_pgrk = [0.400827866,0.863170087,0.389187762,0.924094751,0.157640608,0.714980958,0.216858534,0.237221536,0.076112858,0.841401681]\n",
    "\n",
    "#         doc = []\n",
    "        for index, item in self.result_doc.iterrows():\n",
    "            self.doc.append(Sentence((index+1), item['dokumen'], item['clean_corpus'], item['terms'])) \n",
    "        \n",
    "        #calculate len average\n",
    "        temp_len_doc = 0\n",
    "        for item in self.doc:\n",
    "            temp_len_doc += item.sentence_len\n",
    "        self.slen_ave = temp_len_doc/len(self.doc)\n",
    "        \n",
    "        \n",
    "        #calculate bm25 for each object sentence\n",
    "        for item in self.doc:\n",
    "            for item2 in self.doc:\n",
    "                if item.id is not item2.id:\n",
    "                    item.calculate_bm25(raw_frequency= self.raw_frequency, idf=self.idf, doc=item2, slen_ave=self.slen_ave)\n",
    "           \n",
    "        \n",
    "        ##CHECKING IF BM25 SCORE IS 0 (OUTLIER SENTENCE)\n",
    "        for index, item in enumerate(self.doc):\n",
    "#             print(sum(item.list_bm25.values()))\n",
    "            if sum(item.list_bm25.values()) <= 0:\n",
    "                self.outlier.append(self.doc.pop(index))\n",
    "                \n",
    "                \n",
    "        #calculate pagerank\n",
    "        for i in range(4):\n",
    "            for item in self.doc:\n",
    "                item.calculate_new_pagerank(self.doc)\n",
    "            \n",
    "            #update pagerank score\n",
    "            for item in self.doc:\n",
    "                item.pagerank_score = item.pagerank_score_new\n",
    "        \n",
    "\n",
    "        #getting the summarize        \n",
    "        sorted_doc = sorted(self.doc, key=lambda x: x.pagerank_score, reverse=True)        \n",
    "        top_pagerank = []\n",
    "        for item in range(math.ceil(self.total_doc*self.compression_rate)):\n",
    "            top_pagerank.append(sorted_doc[item])\n",
    "        \n",
    "        sorted_sum = sorted(top_pagerank, key=lambda x: x.id)\n",
    "        \n",
    "        temp_summarize = [item.full_sentence for item in sorted_sum]\n",
    "        \n",
    "        self.summarize = sorted_sum\n",
    "        \n",
    "        \n",
    "#         print()\n",
    "        self.summarize = temp_summarize\n",
    "#         print(self.summarize)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_document = open(\"bbc contoh beneran.txt\", \"r\")\n",
    "document= input_document.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = open(\"stopword_list_tala.txt\", \"r\")\n",
    "stopwords = stopword.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_result = get_clean_corpus(corpus=document, stopwords=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cleaning_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_frequency, df_idf  = get_term_weighting_score(cleaning_result=cleaning_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cr :  0.05\n",
      "\"Melalui pesan digital, Kepala Dinas Pendidikan Kota Palembang menginstruksikan kegiatan belajar mengajar di tingkat paud, TK, SD dan SMP negeri dan swasta diliburkan hingga batas yang belum ditentukan,\" sebut Agus dalam siaran pers.\n",
      "cr :  0.1\n",
      "\"Melalui pesan digital, Kepala Dinas Pendidikan Kota Palembang menginstruksikan kegiatan belajar mengajar di tingkat paud, TK, SD dan SMP negeri dan swasta diliburkan hingga batas yang belum ditentukan,\" sebut Agus dalam siaran pers.\n",
      "cr :  0.2\n",
      "\"Pagi ini kami memulangkan siswa karena melihat kabut asap yang tebal dan berdampak buruk terhadap siswa, oleh karenanya atas instruksi Kadiknas Kota Palembang melalui pesan WA Grup meminta siswa dipulangkan dan belajar di rumah masing-masing saja,\" jelas Siti kepada radio Elshinta.\n",
      "\"Melalui pesan digital, Kepala Dinas Pendidikan Kota Palembang menginstruksikan kegiatan belajar mengajar di tingkat paud, TK, SD dan SMP negeri dan swasta diliburkan hingga batas yang belum ditentukan,\" sebut Agus dalam siaran pers.\n",
      "cr :  0.3\n",
      "Kepala SMP Negeri 7 Palembang, Siti Zubaida, mengatakan keputusan pemulangan ditempuh sesuai dengan instruksi Dinas Pendidikan Kota Palembang.\n",
      "\"Pagi ini kami memulangkan siswa karena melihat kabut asap yang tebal dan berdampak buruk terhadap siswa, oleh karenanya atas instruksi Kadiknas Kota Palembang melalui pesan WA Grup meminta siswa dipulangkan dan belajar di rumah masing-masing saja,\" jelas Siti kepada radio Elshinta.\n",
      "\"Melalui pesan digital, Kepala Dinas Pendidikan Kota Palembang menginstruksikan kegiatan belajar mengajar di tingkat paud, TK, SD dan SMP negeri dan swasta diliburkan hingga batas yang belum ditentukan,\" sebut Agus dalam siaran pers.\n"
     ]
    }
   ],
   "source": [
    "cr = [0.05, 0.10, 0.20, 0.30]\n",
    "for crr in cr:\n",
    "    print(\"cr : \", crr)\n",
    "    percobaan = Graph(result_doc=cleaning_result, raw_frequency=terms_frequency, idf=df_idf, cr=crr)\n",
    "    for ringkasan in percobaan.summarize:\n",
    "        print(ringkasan)\n",
    "    \n",
    "# for item in cobs.summarize:\n",
    "#     print(item.id)\n",
    "#     print(item.full_sentence)\n",
    "#     print(item.pagerank_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coba = \"Indonesia adalah salah satu Negara yang di dalamnya memiliki iklim tropis. Negara ini terdiri dari banyak pulau di setiap wilayahnya. Meskipun harus diakui bahwa di Indonesia sendiri, dataran yang ada tentu tidak seluas lautan yang membentang luas. Namun demikian, Indonesia memiliki kawasan hutan yang cukup banyak mulai dari Sabang yang terletak di provinsi Aceh sampai Merauke yang terletak di kawasan Papua. Namun, di beberapa tahun terakhir, Indonesia sering mengalami kebakaran hutan lantaran beberapa faktor yang ada, mulai dari faktor buatan atau dari manusianya sendiri hingga faktor alam. Faktor alam yang menyebabkan kebakaran hutan memang tidak bisa dihindarkan dan tidak ada yang bisa disalahkan dalam hal ini. Akan tetapi, untuk faktor dari tindakan manusia perlu untuk ditindak dan dievaluasi. Memang sebuah keresahan tersendiri dimana manusia banyak yang kini kehilangan kesadarannya sampai-sampai melakukan suatu perbuatan yang bisa merugikan banyak orang termasuk dirinya sendiri, khususnya merugikan lingkungan hidup. Sedangkan hutan sendiri adalah sejenis habitat yang di dalamnya banyak spesies bergantung. Oleh karena itu, aksi dari manusia dalam menyebabkan kebakaran hutan harus diadili. Terlebih jika itu dengan tujuan untuk kepentingan diri mereka sendiri. Ada banyak alasan yang dimiliki oleh oknum saat melakukan aksi pembakaran hutan, di antaranya adalah untuk pembukaan lahan yang baru atau pembangunan gedung-gedung yang baru dan lain-lain. Akan tetapi, mereka sama sekali tidak memikirkan bagaimana nasib dari flora dan juga fauna yang ada di dalam hutan tersebut. Flora serta fauna yang terdapat di dalam hutan tentu akan melarikan diri. Namun, tentu ada juga yang hangus terbakar api lantaran ulah dari manusia itu sendiri. Mereka tentu akan kehilangan tempat tinggal aslinya. Bahkan akan menjadi keresahan tersendiri jika mereka masuk ke wilayah pemukiman penduduk karena perasaan tidak memiliki rumah untuk tinggal. Maka tidak mengherankan jika akhir-akhir ini ada banyak kasus penemuan hewan liar seperti singa dan macan yang masuk ke pemukiman warga. Berbeda lagi dengan faktor alam misalnya karena kemarau panjang atau adanya sambaran petir kala hujan datang. Musim tentu tidak bisa diperkirakan oleh manusia, sehingga saat kemarau datang dengan masa yang amat panjang adalah hal yang sangat wajar. Akan tetapi, hal tersebut sangat berpengaruh kepada kondisi hutan yang setiap hari terkena sengat matahari menyebabkan munculnya percikan api. Juga karena adanya petir yang menyambar sehingga memunculkan percikan api.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "coba =\"\\\"Ini bisa dicegah, ini bisa dihentikan. Karena ini bukan bencana alam seperti gempa atau tsunami yang datangnya tiba-tiba - hanya Tuhan yang tahu. Tidak,\\\" pungkas Andre.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "judulnya : Kebakaran Hutan yang Terjadi di Indonesia\n",
    "dari https://moondoggiesmusic.com/contoh-artikel/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_coba = get_clean_corpus(corpus=coba, stopwords=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Ini bisa dicegah, ini bisa dihentikan.\n",
      "Karena ini bukan bencana alam seperti gempa atau tsunami yang datangnya tiba-tiba - hanya Tuhan yang tahu.\n",
      "Tidak,\" pungkas Andre.\n"
     ]
    }
   ],
   "source": [
    "for item in cleaning_coba['dokumen']:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_coba, df_idf_coba  = get_term_weighting_score(cleaning_result=cleaning_coba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'cr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-9a83527689dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_doc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcleaning_coba\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_frequency\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf_coba\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf_idf_coba\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummarize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#     print(item.id)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#     print(item.pagerank_score)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'cr'"
     ]
    }
   ],
   "source": [
    "cobs = Graph(result_doc=cleaning_coba, raw_frequency=tf_coba, idf=df_idf_coba)\n",
    "for item in cobs.summarize:\n",
    "#     print(item.id)\n",
    "    print(item.full_sentence)\n",
    "#     print(item.pagerank_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in cobs.outlier:\n",
    "    print(item.full_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_coba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idf_coba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4]\n",
    "a.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
