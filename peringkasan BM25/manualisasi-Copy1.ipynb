{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import time\n",
    "start = time.time()\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "import math\n",
    "\n",
    "#scrapping\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# import StemmerFactory class\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "#term weighting tfidf\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function does tokenize, remove stopwords, and stemming\n",
    "def get_clean_corpus(raw_corpus, stopwords):    \n",
    "    clean_corpus = []\n",
    "    token = []\n",
    "    for index, item in enumerate(corpus['dokumen']):\n",
    "        term = corpus['dokumen'][index].split(\" \")\n",
    "        \n",
    "        #deleting url\n",
    "        deleted_url = [temp for temp in term if not re.match(r\"\\w+(?:(\\.(\\w+)\\.(\\w+)))|\\w+(?:(\\.(\\w+)))\", str(temp))]\n",
    "        \n",
    "        #deleting symbol\n",
    "        deleted_symbol = [re.sub(r\"[\\-\\+\\=\\:\\;\\\"\\\\\\@\\[\\]\\,_!;.':#$%^&*()<>?/\\|}{~:]\",\" \",str(temp)) for temp in deleted_url ]\n",
    "        \n",
    "        #stemming\n",
    "        stemmed_sentence = stemmer.stem(\" \".join(deleted_symbol))\n",
    "        \n",
    "        tokens = stemmed_sentence.split(\" \")\n",
    "        \n",
    "        for i in range(len(tokens)):\n",
    "            for index, word in enumerate(tokens):\n",
    "                #delete stopwprds\n",
    "                if word in stopwords:\n",
    "                    del tokens[index]\n",
    "                    \n",
    "                #delete number\n",
    "                if word.isdigit():\n",
    "                    del tokens[index]\n",
    "        \n",
    "        clean_corpus.append(\" \".join(tokens))  \n",
    "        token.append(list(dict.fromkeys(tokens)))\n",
    "        \n",
    "    raw_corpus['clean_corpus'] = clean_corpus\n",
    "    raw_corpus['token'] = token\n",
    "    \n",
    "    return raw_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function calculates term weighting\n",
    "def get_term_weighting_score(corpus):\n",
    "    #frequency\n",
    "    vectorizer = CountVectorizer(min_df=0)\n",
    "    freq_term_corpus = vectorizer.fit_transform(corpus[\"clean_corpus\"]).toarray()\n",
    "    \n",
    "    \n",
    "    transformer = TfidfTransformer(norm=None, use_idf=True, smooth_idf=True,)\n",
    "    tfidf = transformer.fit_transform(freq_term_corpus)\n",
    "    \n",
    "    #raw weigthing\n",
    "    tokens = vectorizer.get_feature_names()\n",
    "    raw_frequency = pd.DataFrame(freq_term_corpus.transpose())\n",
    "    raw_frequency.columns = [str(item+1) for item in range(len(corpus['dokumen']))]\n",
    "    raw_frequency['tokens'] = tokens\n",
    "    raw_frequency.set_index('tokens', inplace=True)\n",
    "    \n",
    "    #idf\n",
    "    idf = pd.DataFrame({'idf_score':transformer.idf_,\n",
    "                       'tokens':tokens})\n",
    "    idf.set_index('tokens', inplace=True)\n",
    "    \n",
    "    return raw_frequency, idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "    np.random.seed(0)\n",
    "    def __init__(self, id, full_sentence, clean_sentence, tokens):\n",
    "        self.list_bm25 = {}\n",
    "        self.pagerank_score = random.random()\n",
    "#         self.pagerank_score = pagerank\n",
    "        self.id = id\n",
    "        self.full_sentence = full_sentence\n",
    "        self.clean_sentence = clean_sentence\n",
    "        self.tokens = tokens\n",
    "        self.sentence_len = len(clean_sentence.split())\n",
    "        self.pagerank_score_new = 0\n",
    "            \n",
    "    def calculate_bm25(self, raw_frequency, idf, doc, slen_ave):\n",
    "        k1 = 1.2\n",
    "        b = 0.75\n",
    "        total_bm25 = 0\n",
    "        for query in self.tokens:\n",
    "            tf = raw_frequency.at[str(query),str(doc.id)]\n",
    "            idff = idf.at[str(query),'idf_score']\n",
    "            temp = idff * ((k1+1) * tf) / (k1*( (1-b) + (b *(doc.sentence_len/slen_ave)) ) + tf)            \n",
    "            total_bm25 += temp\n",
    "        self.list_bm25[doc.id] = total_bm25\n",
    "#         print(\"IDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD \",self.id)\n",
    "#         print(self.list_bm25)\n",
    "        \n",
    "    def calculate_new_pagerank(self, doc):\n",
    "        d=0.85\n",
    "        sum_InVi = 0\n",
    "        for item in doc:\n",
    "            if self.id is not item.id:\n",
    "                Wji = self.list_bm25[item.id]\n",
    "                total_Wjk = sum(item.list_bm25.values())\n",
    "                sum_InVi += Wji/total_Wjk*item.pagerank_score\n",
    "        self.pagerank_score_new = (1-d)+(d*sum_InVi)\n",
    "#         print(\"IDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD \",self.id)\n",
    "#         print(self.pagerank_score_new)\n",
    "#         print(self.pagerank_score_new)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:    \n",
    "    def __init__(self, result_doc, raw_frequency, idf):\n",
    "        self.raw_frequency = raw_frequency\n",
    "        self.idf = idf\n",
    "        self.result_doc = result_doc\n",
    "        self.total_doc = len(result_doc['dokumen'])\n",
    "        self.slen_ave = 0\n",
    "        self.summarize = []\n",
    "        \n",
    "        \n",
    "        #making list of object sentence\n",
    "        doc = []\n",
    "        for index, item in self.result_doc.iterrows():\n",
    "            doc.append(Sentence((index+1), item['dokumen'], item['clean_corpus'], item['token']))\n",
    "        \n",
    "        #calculate len average\n",
    "        temp_len_doc = 0\n",
    "        for item in doc:\n",
    "            temp_len_doc += item.sentence_len\n",
    "        self.slen_ave = temp_len_doc/len(doc)\n",
    "        \n",
    "        \n",
    "        #calculate bm25 for each object sentence\n",
    "        for item in doc:\n",
    "            for item2 in doc:\n",
    "                if item.id is not item2.id:\n",
    "                    item.calculate_bm25(raw_frequency= self.raw_frequency, idf=self.idf, doc=item2, slen_ave=self.slen_ave)\n",
    "                    \n",
    "        #calculate pagerank\n",
    "        for i in range(100):\n",
    "            for item in doc:\n",
    "                item.calculate_new_pagerank(doc)\n",
    "            \n",
    "            #update pagerank score\n",
    "            for item in doc:\n",
    "                item.pagerank_score = item.pagerank_score_new\n",
    "        \n",
    "\n",
    "        #getting the summarize        \n",
    "        sorted_doc = sorted(doc, key=lambda x: x.pagerank_score, reverse=True)        \n",
    "        top_pagerank = []\n",
    "        for item in range(math.ceil(self.total_doc*0.25)):\n",
    "            top_pagerank.append(sorted_doc[item])\n",
    "        \n",
    "        sorted_sum = sorted(top_pagerank, key=lambda x: x.id)\n",
    "        \n",
    "        temp_summarize = [item.full_sentence for item in sorted_sum]\n",
    "        \n",
    "        self.summarize = sorted_sum\n",
    "#         self.summarize = temp_summarize\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopword = open(\"stopword_list_tala.txt\", \"r\")\n",
    "stopwords = stopword.read().split(\"\\n\")\n",
    "corpus = pd.read_csv(\"coba2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_doc = get_clean_corpus(corpus, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_frequency, idf = get_term_weighting_score(result_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Sejumlah sekolah di Kota Palembang, Sumatera Selatan, memutuskan memulangkan siswa-siswa mereka lantaran kabut asap semakin tebal menyelimuti kota tersebut. pada Senin (14/10) pagi.\n",
      "1.5149792748244266\n",
      "3\n",
      "Pagi ini kami memulangkan siswa karena melihat kabut asap yang tebal dan berdampak buruk terhadap siswa, oleh karenanya atas instruksi Kadiknas Kota Palembang melalui pesan WA Grup meminta siswa dipulangkan dan belajar di rumah masing-masing saja, jelas Siti kepada radio?Elshinta.\n",
      "1.7430588301054804\n",
      "7\n",
      "Akan tetapi, sebagaimana dipaparkan Kepala Dinas Pendidikan Sumatera Selatan, Widodo, kegiatan belajar mengajar di daerah yang tidak terdampak kabut asap tetap berlangsung.\n",
      "1.4938766458706758\n",
      "10\n",
      "Melalui pesan digital, Kepala Dinas Pendidikan Kota Palembang menginstruksikan kegiatan belajar mengajar di tingkat paud, TK, SD dan SMP negeri dan swasta diliburkan hingga batas yang belum ditentukan, sebut Agus dalam siaran pers.\n",
      "1.7431166059102274\n",
      "11\n",
      "Sejumlah warga Palembang, Sumatera Selatan, mengeluhkan kabut asap pada Senin (14/10) tergolong parah. Bahkan jarak pandang hanya 10 meter.\n",
      "1.4537262003063036\n",
      "16\n",
      "Kepala Seksi Observasi dan Informasi Stasiun Meteorologi SMB II Palembang, Bambang Beny Setiaji, mengatakan kabut tersebut bercampur asap kiriman dari wilayah Kabupaten Ogan Komering Ilir (OKI) yang berada sebelah tenggara Kota Palembang.\n",
      "1.6937334898631358\n",
      "18\n",
      "Kabut asap di Kota Palembang semakin parah dalam sepekan terakhir akibat dampak kebakaran hutan dan lahan di sejumlah kabupaten.\n",
      "1.602564575330128\n",
      "24\n",
      "Sementara itu, aktivitas kapal bertonase di Sungai Musi, Kota Palembang, dihentikan akibat kabut asap pekat.\n",
      "1.5619113685005503\n"
     ]
    }
   ],
   "source": [
    "cobs = Graph(result_doc=result_doc, raw_frequency=raw_frequency, idf=idf)\n",
    "# cobs.summarize\n",
    "for item in cobs.summarize:\n",
    "    print(item.id)\n",
    "    print(item.full_sentence)\n",
    "    print(item.pagerank_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.469436645507812\n"
     ]
    }
   ],
   "source": [
    "elapsed_time_fl = (time.time() - start) \n",
    "print(elapsed_time_fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'www ini makam co id bukan juga 24 7 00 00 00 00ya'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yap = stemmer.stem('www.ini_makam.co.id bukan juga 24/7 00.00 00,00ya!')\n",
    "yap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(pd.__version__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for item in range(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.ceil(1.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
