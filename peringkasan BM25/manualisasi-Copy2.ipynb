{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import time\n",
    "start = time.time()\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "import math\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#scrapping\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# import StemmerFactory class\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "#term weighting tfidf\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function does tokenize, remove stopwords, and stemming\n",
    "def get_clean_corpus(raw_corpus, stopwords):    \n",
    "    clean_corpus = []\n",
    "    token = []\n",
    "    for index, item in enumerate(corpus['dokumen']):\n",
    "        term = corpus['dokumen'][index].split(\" \")\n",
    "        \n",
    "        #deleting url\n",
    "        deleted_url = [temp for temp in term if not re.match(r\"\\w+(?:(\\.(\\w+)\\.(\\w+)))|\\w+(?:(\\.(\\w+)))\", str(temp))]\n",
    "        \n",
    "        #deleting symbol\n",
    "        deleted_symbol = [re.sub(r\"[\\-\\+\\=\\:\\;\\\"\\\\\\@\\[\\]\\,_!;.':#$%^&*()<>?/\\|}{~:]\",\" \",str(temp)) for temp in deleted_url ]\n",
    "        \n",
    "        #stemming\n",
    "        stemmed_sentence = stemmer.stem(\" \".join(deleted_symbol))\n",
    "        \n",
    "        tokens = word_tokenize(stemmed_sentence)\n",
    "        \n",
    "        for i in range(len(tokens)):\n",
    "            for index, word in enumerate(tokens):\n",
    "                #delete stopwprds\n",
    "                if word in stopwords:\n",
    "                    del tokens[index]\n",
    "                    \n",
    "                #delete number\n",
    "                if word.isdigit():\n",
    "                    del tokens[index]\n",
    "        \n",
    "        clean_corpus.append(\" \".join(tokens))  \n",
    "        token.append(list(dict.fromkeys(tokens)))\n",
    "        \n",
    "    raw_corpus['clean_corpus'] = clean_corpus\n",
    "    raw_corpus['token'] = token\n",
    "    \n",
    "    return raw_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function calculates term weighting\n",
    "def get_term_weighting_score(corpus):\n",
    "    #frequency\n",
    "    vectorizer = CountVectorizer(min_df=0)\n",
    "    freq_term_corpus = vectorizer.fit_transform(corpus[\"clean_corpus\"]).toarray()\n",
    "    \n",
    "    #raw weigthing\n",
    "    transformer = TfidfTransformer(norm=None, use_idf=True, smooth_idf=True,)\n",
    "    tfidf = transformer.fit_transform(freq_term_corpus)       \n",
    "    tokens = vectorizer.get_feature_names()\n",
    "    raw_frequency = pd.DataFrame(freq_term_corpus.transpose())\n",
    "    raw_frequency.columns = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "    raw_frequency['tokens'] = tokens\n",
    "    raw_frequency.set_index('tokens', inplace=True)\n",
    "    \n",
    "    #idf\n",
    "    idf = pd.DataFrame({'idf_score':transformer.idf_,\n",
    "                       'tokens':tokens})\n",
    "    idf.set_index('tokens', inplace=True)\n",
    "    \n",
    "    return raw_frequency, idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "    np.random.seed(0)\n",
    "    def __init__(self, id, full_sentence, clean_sentence, tokens, pagerank):\n",
    "        self.list_bm25 = {}\n",
    "#         self.pagerank_score = random.random()\n",
    "        self.pagerank_score = pagerank\n",
    "        self.id = id\n",
    "        self.full_sentence = full_sentence\n",
    "        self.clean_sentence = clean_sentence\n",
    "        self.tokens = tokens\n",
    "        self.sentence_len = len(clean_sentence.split())\n",
    "        self.pagerank_score_new = 0\n",
    "            \n",
    "    def calculate_bm25(self, raw_frequency, idf, doc, slen_ave):\n",
    "        k1 = 1.2\n",
    "        b = 0.75\n",
    "        total_bm25 = 0\n",
    "        for query in self.tokens:\n",
    "            tf = raw_frequency.at[str(query),str(doc.id)]\n",
    "            idff = idf.at[str(query),'idf_score']\n",
    "            temp = idff * ((k1+1) * tf) / (k1*( (1-b) + (b *(doc.sentence_len/slen_ave)) ) + tf)            \n",
    "            total_bm25 += temp\n",
    "        self.list_bm25[doc.id] = total_bm25\n",
    "        \n",
    "    def calculate_new_pagerank(self, doc):\n",
    "        d=0.85\n",
    "        sum_InVi = 0\n",
    "        for item in doc:\n",
    "            if self.id is not item.id:\n",
    "                Wji = self.list_bm25[item.id]\n",
    "                total_Wjk = sum(item.list_bm25.values())\n",
    "                sum_InVi += Wji/total_Wjk*item.pagerank_score\n",
    "        self.pagerank_score_new = (1-d)+(d*sum_InVi)\n",
    "#         print(self.pagerank_score_new)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:    \n",
    "    def __init__(self, result_doc, raw_frequency, idf):\n",
    "        self.raw_frequency = raw_frequency\n",
    "        self.idf = idf\n",
    "        self.result_doc = result_doc\n",
    "        self.total_doc = len(result_doc['dokumen'])\n",
    "        self.slen_ave = 0\n",
    "        self.summarize = []\n",
    "        \n",
    "        \n",
    "        #making object sentence\n",
    "        list_pgrk = [0.400827866,0.863170087,0.389187762,0.924094751,0.157640608,0.714980958,0.216858534,0.237221536,0.076112858,0.841401681]\n",
    "\n",
    "        doc = []\n",
    "        for index, item in self.result_doc.iterrows():\n",
    "            doc.append(Sentence((index+1), item['dokumen'], item['clean_corpus'], item['token'], pagerank=list_pgrk[index])) \n",
    "        \n",
    "        #calculate len average\n",
    "        temp_len_doc = 0\n",
    "        for item in doc:\n",
    "            temp_len_doc += item.sentence_len\n",
    "        self.slen_ave = temp_len_doc/len(doc)\n",
    "        \n",
    "        \n",
    "        #calculate bm25 for each object sentence\n",
    "        for item in doc:\n",
    "            for item2 in doc:\n",
    "                if item.id is not item2.id:\n",
    "                    item.calculate_bm25(raw_frequency= self.raw_frequency, idf=self.idf, doc=item2, slen_ave=self.slen_ave)\n",
    "                    \n",
    "        #calculate pagerank\n",
    "        for i in range(20):\n",
    "            for item in doc:\n",
    "                item.calculate_new_pagerank(doc)\n",
    "            \n",
    "            #update pagerank score\n",
    "            for item in doc:\n",
    "                item.pagerank_score = item.pagerank_score_new\n",
    "        \n",
    "\n",
    "        #getting the summarize        \n",
    "        sorted_doc = sorted(doc, key=lambda x: x.pagerank_score, reverse=True)        \n",
    "        top_pagerank = []\n",
    "        for item in range(math.ceil(self.total_doc*0.25)):\n",
    "            top_pagerank.append(sorted_doc[item])\n",
    "        \n",
    "        sorted_sum = sorted(top_pagerank, key=lambda x: x.id)\n",
    "        \n",
    "        temp_summarize = [item.full_sentence for item in sorted_sum]\n",
    "        \n",
    "        self.summarize = sorted_sum\n",
    "#         self.summarize = temp_summarize\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = open(\"stopword_list_tala.txt\", \"r\")\n",
    "stopwords = stopword.read().split(\"\\n\")\n",
    "corpus = pd.read_csv(\"corpus2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_doc = get_clean_corpus(corpus, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_frequency, idf = get_term_weighting_score(result_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idf_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>agus</th>\n",
       "      <td>2.299283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ahmad</th>\n",
       "      <td>2.299283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ajar</th>\n",
       "      <td>1.606136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amin</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asap</th>\n",
       "      <td>1.788457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>badan</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batas</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bencana</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>berita</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>besok</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bnpb</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buruk</th>\n",
       "      <td>2.299283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daerah</th>\n",
       "      <td>2.299283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dampak</th>\n",
       "      <td>2.011601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daring</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>derajat</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>didik</th>\n",
       "      <td>1.788457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>digital</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dinas</th>\n",
       "      <td>1.788457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edar</th>\n",
       "      <td>2.299283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elshinta</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>giat</th>\n",
       "      <td>2.011601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grup</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hadap</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>himbau</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>humas</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>informasi</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruksi</th>\n",
       "      <td>2.011601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jam</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pulang</th>\n",
       "      <td>2.011601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pusat</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>putus</th>\n",
       "      <td>2.299283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radio</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rumah</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sd</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sekolah</th>\n",
       "      <td>2.011601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>selatan</th>\n",
       "      <td>2.299283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>senin</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sesuai</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>siar</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>siswa</th>\n",
       "      <td>2.011601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>siti</th>\n",
       "      <td>2.299283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smp</th>\n",
       "      <td>1.788457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sumatera</th>\n",
       "      <td>2.299283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surat</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swasta</th>\n",
       "      <td>2.299283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tanggulang</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tebal</th>\n",
       "      <td>2.299283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tempuh</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tingkat</th>\n",
       "      <td>2.299283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tk</th>\n",
       "      <td>2.011601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tugas</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>udara</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>undur</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wa</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wibowo</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>widodo</th>\n",
       "      <td>2.299283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zubaida</th>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zulinto</th>\n",
       "      <td>2.299283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            idf_score\n",
       "tokens               \n",
       "agus         2.299283\n",
       "ahmad        2.299283\n",
       "ajar         1.606136\n",
       "amin         2.704748\n",
       "asap         1.788457\n",
       "badan        2.704748\n",
       "batas        2.704748\n",
       "bencana      2.704748\n",
       "berita       2.704748\n",
       "besok        2.704748\n",
       "bnpb         2.704748\n",
       "buruk        2.299283\n",
       "daerah       2.299283\n",
       "dampak       2.011601\n",
       "daring       2.704748\n",
       "data         2.704748\n",
       "derajat      2.704748\n",
       "didik        1.788457\n",
       "digital      2.704748\n",
       "dinas        1.788457\n",
       "edar         2.299283\n",
       "elshinta     2.704748\n",
       "giat         2.011601\n",
       "grup         2.704748\n",
       "hadap        2.704748\n",
       "himbau       2.704748\n",
       "humas        2.704748\n",
       "informasi    2.704748\n",
       "instruksi    2.011601\n",
       "jam          2.704748\n",
       "...               ...\n",
       "pulang       2.011601\n",
       "pusat        2.704748\n",
       "putus        2.299283\n",
       "radio        2.704748\n",
       "rumah        2.704748\n",
       "sd           2.704748\n",
       "sekolah      2.011601\n",
       "selatan      2.299283\n",
       "senin        2.704748\n",
       "sesuai       2.704748\n",
       "siar         2.704748\n",
       "siswa        2.011601\n",
       "siti         2.299283\n",
       "smp          1.788457\n",
       "sumatera     2.299283\n",
       "surat        2.704748\n",
       "swasta       2.299283\n",
       "tanggulang   2.704748\n",
       "tebal        2.299283\n",
       "tempuh       2.704748\n",
       "tingkat      2.299283\n",
       "tk           2.011601\n",
       "tugas        2.704748\n",
       "udara        2.704748\n",
       "undur        2.704748\n",
       "wa           2.704748\n",
       "wibowo       2.704748\n",
       "widodo       2.299283\n",
       "zubaida      2.704748\n",
       "zulinto      2.299283\n",
       "\n",
       "[87 rows x 1 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Kepala SMP Negeri 7 Palembang, Siti Zubaida, mengatakan keputusan pemulangan ditempuh sesuai dengan instruksi Dinas Pendidikan Kota Palembang.\n",
      "1.1796589813966847\n",
      "3\n",
      "\"Pagi ini kami memulangkan siswa karena melihat kabut asap yang tebal dan berdampak buruk terhadap siswa, oleh karenanya atas instruksi Kadiknas Kota Palembang melalui pesan WA Grup meminta siswa dipulangkan dan belajar di rumah masing-masing saja,\" jelas Siti kepada radio Elshinta.\n",
      "1.3132836272663058\n",
      "10\n",
      "\"Melalui pesan digital, Kepala Dinas Pendidikan Kota Palembang menginstruksikan kegiatan belajar mengajar di tingkat paud, TK, SD dan SMP negeri dan swasta diliburkan hingga batas yang belum ditentukan,\" sebut Agus dalam siaran pers.\n",
      "1.7250902614023773\n"
     ]
    }
   ],
   "source": [
    "cobs = Graph(result_doc=result_doc, raw_frequency=raw_frequency, idf=idf)\n",
    "for item in cobs.summarize:\n",
    "    print(item.id)\n",
    "    print(item.full_sentence)\n",
    "    print(item.pagerank_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.483185052871704\n"
     ]
    }
   ],
   "source": [
    "elapsed_time_fl = (time.time() - start) \n",
    "print(elapsed_time_fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'www ini makam co id bukan juga 24 7 00 00 00 00ya'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yap = stemmer.stem('www.ini_makam.co.id bukan juga 24/7 00.00 00,00ya!')\n",
    "yap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(pd.__version__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for item in range(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.ceil(1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Serangan roket diarahkan ke wilayah Israel bagian selatan dari Gaza setelah terbunuhnya Al-Ata.',\n",
       " 'Adapun PIJ sudah bersumpah untuk membalaskan kematian pemimpinnya.',\n",
       " 'Sejauh ini tidak ada laporan langsung korban luka atau tewas akibat serangan roket di wilayah Israel bagian selatan.',\n",
       " 'PIJ, yang didukung Iran, adalah kelompok militan terbesar kedua di Gaza dan sejauh ini telah melakukan banyak serangan roket ke Israel.',\n",
       " 'Baha Abu al-Ata, pemimpin Palestinian Islamic Jihad (PIJ), terbunuh bersama istrinya, ketika rudal menghantam rumah mereka, kata kelompok itu.',\n",
       " 'Israel mengatakan Al-Ata adalah \"bom yang siap meledak\" yang merencanakan \"serangan teroris dalam waktu dekat\".',\n",
       " 'Pada waktu yang hampir bersamaan dengan tewasnya Al-Ata, sosok senior lain PIJ juga tewas dalam serangan roket Israel di rumahnya di ibu kota Suriah, Damaskus, ungkap kantor berita Suriah, Sana.',\n",
       " 'Akram al-Ajouri tewas bersama dengan putranya dalam serangan tersebut, demikian laporan Sana.',\n",
       " 'Israel belum mengomentari insiden itu.',\n",
       " 'Kelompok militan Palestina Hamas, yang menguasai Jalur Gaza dan dianggap sebagai saingan PIJ, mengutuk keras pembunuhan Al-Ata, bersumpah \"tidak akan membiarkan kasus itu berlalu tanpa hukuman\".',\n",
       " 'Serangan yang terjadi pada Senin malam menandai kenaikan eskalasi dalam konflik Israel dan kelompok-kelompok militan Palestina, yang saling bertarung dalam peperangan dan kekerasan lintas-perbatasan selama bertahun-tahun.',\n",
       " 'Mr. Sorekarno pernah melakukan kunjungan ke dunia.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(ccc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"coba bbc.txt\", \"r\")\n",
    "ccc= f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Serangan roket diarahkan ke wilayah Israel bagian selatan dari Gaza setelah terbunuhnya Al-Ata. Adapun PIJ sudah bersumpah untuk membalaskan kematian pemimpinnya. Sejauh ini tidak ada laporan langsung korban luka atau tewas akibat serangan roket di wilayah Israel bagian selatan. PIJ, yang didukung Iran, adalah kelompok militan terbesar kedua di Gaza dan sejauh ini telah melakukan banyak serangan roket ke Israel. Baha Abu al-Ata, pemimpin Palestinian Islamic Jihad (PIJ), terbunuh bersama istrinya, ketika rudal menghantam rumah mereka, kata kelompok itu. Israel mengatakan Al-Ata adalah \"bom yang siap meledak\" yang merencanakan \"serangan teroris dalam waktu dekat\". Pada waktu yang hampir bersamaan dengan tewasnya Al-Ata, sosok senior lain PIJ juga tewas dalam serangan roket Israel di rumahnya di ibu kota Suriah, Damaskus, ungkap kantor berita Suriah, Sana. Akram al-Ajouri tewas bersama dengan putranya dalam serangan tersebut, demikian laporan Sana. Israel belum mengomentari insiden itu. Kelompok militan Palestina Hamas, yang menguasai Jalur Gaza dan dianggap sebagai saingan PIJ, mengutuk keras pembunuhan Al-Ata, bersumpah \"tidak akan membiarkan kasus itu berlalu tanpa hukuman\". Serangan yang terjadi pada Senin malam menandai kenaikan eskalasi dalam konflik Israel dan kelompok-kelompok militan Palestina, yang saling bertarung dalam peperangan dan kekerasan lintas-perbatasan selama bertahun-tahun. Mr. Sorekarno pernah melakukan kunjungan ke dunia.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ccc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
